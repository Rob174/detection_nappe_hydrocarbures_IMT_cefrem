<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>main.src.analysis.tools API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main.src.analysis.tools</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import cv2
import numpy as np
import torch
import rich
import matplotlib.pyplot as plt
from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn

from main.FolderInfos import FolderInfos
from main.src.data.DatasetFactory import DatasetFactory



class RGB_Overlay_Patch:
    def __init__(self,dataset_name=&#34;classificationpatch&#34;,usage_type=&#34;classification&#34;,patch_creator=&#34;fixed_px&#34;,
                 grid_size=1000,input_size=256,classes_to_use=&#34;other,seep,spill&#34;):
        FolderInfos.init(test_without_data=False)
        self.dataset = DatasetFactory(dataset_name=dataset_name, usage_type=usage_type, patch_creator=patch_creator,
                                      grid_size=grid_size , input_size=input_size,classes_to_use=classes_to_use)

    def __call__(self,name_img,model,blending_factor=0.5,device=None):
        return self.call(name_img,model,blending_factor,device)
    def call(self,name_img,model,blending_factor=0.5,device=None):
        &#34;&#34;&#34;In this function we will constitute patch after patch the overlay of the image filling true and prediction empty matrices with corresponding patches

        Args:
            name_img: id / name of the image as shown in images_informations_preprocessed.json (better to vizualize but the &#34;official&#34; keys are in the images_preprocessed.hdf5 file)
            model: a created model with pretrained weights already loaded ⚠️
            blending_factor: the percentage (∈ [0.,1.]) of importance given to the color
            device: a pytorch device (gpu)

        Returns: tuple, with  overlay_true,overlay_pred,original_img as np arrays

        &#34;&#34;&#34;
        original_img = self.dataset.attr_dataset.images[name_img] # radar image input (1 channel only)
        overlay_true = np.zeros((*original_img.shape,3),dtype=np.float32) # prepare the overlays with 3 channels for colors
        overlay_pred = np.zeros((*original_img.shape,3),dtype=np.float32)
        normalize = lambda x:(x-np.min(original_img))/(np.max(original_img)-np.min(original_img))
        # get the list of patches organized as follows [[input_patch0,output_patch0,filter_patch0],....
        patches = self.dataset.attr_dataset.make_patches_of_image(name_img)

        progress = Progress(
            TextColumn(&#34;{task.fields[name]}&#34;, justify=&#34;right&#34;),
            BarColumn(bar_width=None),
            &#34;[progress.percentage]{task.percentage:&gt;3.1f}%&#34;,
            &#34;•&#34;,
            TimeElapsedColumn(),
            &#34;•&#34;,
            TimeRemainingColumn()
        ) # prepare the progress bar -&gt; tell which columns to display
        with progress: # progress bar manager to use the progress bar
            progression = progress.add_task(&#34;generation&#34;, name=&#34;[red]Progress&#34;, total=len(patches))
            for id,[input,output,filter] in enumerate(patches):
                if filter is True: # skip the image if the dataset ask to skip this patch (can be for multiple reasons -&gt; see DatasetFactory parameters supplied)
                    continue
                input_adapted = np.stack((input,input,input),axis=0) # convert patch to rgb
                # pytorch can only make predictions for batches of images. That is why we create a &#34;batch&#34; of one image by adding one dimension to the image:
                #shape : (1, img_width, img_height, 3)
                input_adapted = input_adapted.reshape((1,*input_adapted.shape))
                # predict output with cpu if no device (gpu) is provided else predict with gpu and ransfer the result on cpu
                prediction = model(input) if device is None else model(torch.tensor(input_adapted).to(device)).cpu()
                # get the pixel position of the patch
                pos_x,pos_y = self.dataset.attr_patch_creator.get_position_patch(id,original_img.shape)
                if len(output) &gt; 3:
                    raise Exception(f&#34;{len(output)} classes : Too much : cannot use this vizualization technic&#34;)
                input = normalize(input)
                resized_grid_size = self.dataset.attr_patch_creator.attr_grid_size_px
                input = np.stack((input,)*3,axis=-1) # convert in rgb. NB: (input,)*3 &lt;=&gt; (input,input,input)
                # initialize the overlay for the patch
                color_true = np.ones((resized_grid_size,resized_grid_size,3))
                color_pred = np.ones((resized_grid_size,resized_grid_size,3))
                for i,c in enumerate(output):
                    color_true[:,:,i] *= c
                if device is not None:
                    prediction = prediction.cpu().detach().numpy()
                for i,c in enumerate(prediction[0]):

                    color_pred[:,:,i] *= c

                coordx1_not_resize = pos_x
                # coordx1 = int(transformation_matrix.dot(np.array([coordx1_not_resize,0,1]))[0])
                coordx2_not_resize = coordx1_not_resize + self.dataset.attr_patch_creator.attr_grid_size_px
                # coordx2 = int(transformation_matrix.dot(np.array([coordx2_not_resize,0,1]))[0])
                coordy1_not_resize = pos_y
                # coordy1 = int(transformation_matrix.dot(np.array([0,coordy1_not_resize,1]))[1])
                coordy2_not_resize = coordy1_not_resize + self.dataset.attr_patch_creator.attr_grid_size_px
                # coordy2 = int(transformation_matrix.dot(np.array([0,coordy2_not_resize,1]))[1])
                overlay_true[coordx1_not_resize:coordx2_not_resize,coordy1_not_resize:coordy2_not_resize,:] = input * (1-blending_factor) + color_true * blending_factor
                overlay_pred[coordx1_not_resize:coordx2_not_resize,coordy1_not_resize:coordy2_not_resize,:] = input * (1-blending_factor) + color_pred * blending_factor
                progress.update(progression, advance=1)
        return overlay_true,overlay_pred,original_img

if __name__ == &#34;__main__&#34;:
    choice_folder1 = &#39;2021-06-19_04h32min09s_&#39;
    from main.src.models.ModelFactory import ModelFactory
    import json

    FolderInfos.init(test_without_data=True)
    folder = FolderInfos.data_folder + choice_folder1 + FolderInfos.separator
    with open(folder + choice_folder1 + &#34;parameters.json&#34;, &#34;r&#34;) as fp:
        dico = json.load(fp)

    rgb_overlay = RGB_Overlay_Patch(dataset_name=&#34;classificationpatch1&#34;, usage_type=&#34;classification&#34;,
                                    patch_creator=&#34;fixed_px&#34;,
                                    grid_size=dico[&#34;data&#34;][&#34;attr_patch_creator&#34;][&#34;attr_grid_size_px&#34;],
                                    input_size=dico[&#34;data&#34;][&#34;attr_dataset&#34;][&#34;attr_resizer&#34;][
                                        &#34;attr_out_size_w&#34;],
                                    classes_to_use=dico[&#34;data&#34;][&#34;attr_dataset&#34;][&#34;attr_classes_to_use&#34;]
                                    )
    epoch = 0
    iteration = 60835
    import torch

    name = &#34;027481_0319CB_0EB7&#34;

    device = torch.device(&#34;cuda&#34;) # get the gpu
    model = ModelFactory(model_name=dico[&#34;model&#34;][&#34;attr_model_name&#34;], num_classes=dico[&#34;model&#34;][&#34;attr_num_classes&#34;])() # create and get the model
    model.to(device) # put it on the gpu
    model.load_state_dict(torch.load(f&#34;{folder}{choice_folder1}_model_epoch-{epoch}_it-{iteration}.pt&#34;)) # load pretrained weights
    array_overlay = rgb_overlay(name_img=name, model=model, blending_factor=0.5, device=device) # create the overlay (use method __call__)

    import matplotlib.pyplot as plt
    # show the result in two separated figures
    plt.figure(figsize=(10,10))
    plt.imshow(array_overlay[0])
    plt.figure(figsize=(10,10))
    plt.imshow(array_overlay[1])
    plt.show()
    print(array_overlay)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="main.src.analysis.tools.RGB_Overlay_Patch"><code class="flex name class">
<span>class <span class="ident">RGB_Overlay_Patch</span></span>
<span>(</span><span>dataset_name='classificationpatch', usage_type='classification', patch_creator='fixed_px', grid_size=1000, input_size=256, classes_to_use='other,seep,spill')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RGB_Overlay_Patch:
    def __init__(self,dataset_name=&#34;classificationpatch&#34;,usage_type=&#34;classification&#34;,patch_creator=&#34;fixed_px&#34;,
                 grid_size=1000,input_size=256,classes_to_use=&#34;other,seep,spill&#34;):
        FolderInfos.init(test_without_data=False)
        self.dataset = DatasetFactory(dataset_name=dataset_name, usage_type=usage_type, patch_creator=patch_creator,
                                      grid_size=grid_size , input_size=input_size,classes_to_use=classes_to_use)

    def __call__(self,name_img,model,blending_factor=0.5,device=None):
        return self.call(name_img,model,blending_factor,device)
    def call(self,name_img,model,blending_factor=0.5,device=None):
        &#34;&#34;&#34;In this function we will constitute patch after patch the overlay of the image filling true and prediction empty matrices with corresponding patches

        Args:
            name_img: id / name of the image as shown in images_informations_preprocessed.json (better to vizualize but the &#34;official&#34; keys are in the images_preprocessed.hdf5 file)
            model: a created model with pretrained weights already loaded ⚠️
            blending_factor: the percentage (∈ [0.,1.]) of importance given to the color
            device: a pytorch device (gpu)

        Returns: tuple, with  overlay_true,overlay_pred,original_img as np arrays

        &#34;&#34;&#34;
        original_img = self.dataset.attr_dataset.images[name_img] # radar image input (1 channel only)
        overlay_true = np.zeros((*original_img.shape,3),dtype=np.float32) # prepare the overlays with 3 channels for colors
        overlay_pred = np.zeros((*original_img.shape,3),dtype=np.float32)
        normalize = lambda x:(x-np.min(original_img))/(np.max(original_img)-np.min(original_img))
        # get the list of patches organized as follows [[input_patch0,output_patch0,filter_patch0],....
        patches = self.dataset.attr_dataset.make_patches_of_image(name_img)

        progress = Progress(
            TextColumn(&#34;{task.fields[name]}&#34;, justify=&#34;right&#34;),
            BarColumn(bar_width=None),
            &#34;[progress.percentage]{task.percentage:&gt;3.1f}%&#34;,
            &#34;•&#34;,
            TimeElapsedColumn(),
            &#34;•&#34;,
            TimeRemainingColumn()
        ) # prepare the progress bar -&gt; tell which columns to display
        with progress: # progress bar manager to use the progress bar
            progression = progress.add_task(&#34;generation&#34;, name=&#34;[red]Progress&#34;, total=len(patches))
            for id,[input,output,filter] in enumerate(patches):
                if filter is True: # skip the image if the dataset ask to skip this patch (can be for multiple reasons -&gt; see DatasetFactory parameters supplied)
                    continue
                input_adapted = np.stack((input,input,input),axis=0) # convert patch to rgb
                # pytorch can only make predictions for batches of images. That is why we create a &#34;batch&#34; of one image by adding one dimension to the image:
                #shape : (1, img_width, img_height, 3)
                input_adapted = input_adapted.reshape((1,*input_adapted.shape))
                # predict output with cpu if no device (gpu) is provided else predict with gpu and ransfer the result on cpu
                prediction = model(input) if device is None else model(torch.tensor(input_adapted).to(device)).cpu()
                # get the pixel position of the patch
                pos_x,pos_y = self.dataset.attr_patch_creator.get_position_patch(id,original_img.shape)
                if len(output) &gt; 3:
                    raise Exception(f&#34;{len(output)} classes : Too much : cannot use this vizualization technic&#34;)
                input = normalize(input)
                resized_grid_size = self.dataset.attr_patch_creator.attr_grid_size_px
                input = np.stack((input,)*3,axis=-1) # convert in rgb. NB: (input,)*3 &lt;=&gt; (input,input,input)
                # initialize the overlay for the patch
                color_true = np.ones((resized_grid_size,resized_grid_size,3))
                color_pred = np.ones((resized_grid_size,resized_grid_size,3))
                for i,c in enumerate(output):
                    color_true[:,:,i] *= c
                if device is not None:
                    prediction = prediction.cpu().detach().numpy()
                for i,c in enumerate(prediction[0]):

                    color_pred[:,:,i] *= c

                coordx1_not_resize = pos_x
                # coordx1 = int(transformation_matrix.dot(np.array([coordx1_not_resize,0,1]))[0])
                coordx2_not_resize = coordx1_not_resize + self.dataset.attr_patch_creator.attr_grid_size_px
                # coordx2 = int(transformation_matrix.dot(np.array([coordx2_not_resize,0,1]))[0])
                coordy1_not_resize = pos_y
                # coordy1 = int(transformation_matrix.dot(np.array([0,coordy1_not_resize,1]))[1])
                coordy2_not_resize = coordy1_not_resize + self.dataset.attr_patch_creator.attr_grid_size_px
                # coordy2 = int(transformation_matrix.dot(np.array([0,coordy2_not_resize,1]))[1])
                overlay_true[coordx1_not_resize:coordx2_not_resize,coordy1_not_resize:coordy2_not_resize,:] = input * (1-blending_factor) + color_true * blending_factor
                overlay_pred[coordx1_not_resize:coordx2_not_resize,coordy1_not_resize:coordy2_not_resize,:] = input * (1-blending_factor) + color_pred * blending_factor
                progress.update(progression, advance=1)
        return overlay_true,overlay_pred,original_img</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="main.src.analysis.tools.RGB_Overlay_Patch.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, name_img, model, blending_factor=0.5, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>In this function we will constitute patch after patch the overlay of the image filling true and prediction empty matrices with corresponding patches</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name_img</code></strong></dt>
<dd>id / name of the image as shown in images_informations_preprocessed.json (better to vizualize but the "official" keys are in the images_preprocessed.hdf5 file)</dd>
<dt><strong><code>model</code></strong></dt>
<dd>a created model with pretrained weights already loaded ⚠️</dd>
<dt><strong><code>blending_factor</code></strong></dt>
<dd>the percentage (∈ [0.,1.]) of importance given to the color</dd>
<dt><strong><code>device</code></strong></dt>
<dd>a pytorch device (gpu)</dd>
</dl>
<p>Returns: tuple, with
overlay_true,overlay_pred,original_img as np arrays</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self,name_img,model,blending_factor=0.5,device=None):
    &#34;&#34;&#34;In this function we will constitute patch after patch the overlay of the image filling true and prediction empty matrices with corresponding patches

    Args:
        name_img: id / name of the image as shown in images_informations_preprocessed.json (better to vizualize but the &#34;official&#34; keys are in the images_preprocessed.hdf5 file)
        model: a created model with pretrained weights already loaded ⚠️
        blending_factor: the percentage (∈ [0.,1.]) of importance given to the color
        device: a pytorch device (gpu)

    Returns: tuple, with  overlay_true,overlay_pred,original_img as np arrays

    &#34;&#34;&#34;
    original_img = self.dataset.attr_dataset.images[name_img] # radar image input (1 channel only)
    overlay_true = np.zeros((*original_img.shape,3),dtype=np.float32) # prepare the overlays with 3 channels for colors
    overlay_pred = np.zeros((*original_img.shape,3),dtype=np.float32)
    normalize = lambda x:(x-np.min(original_img))/(np.max(original_img)-np.min(original_img))
    # get the list of patches organized as follows [[input_patch0,output_patch0,filter_patch0],....
    patches = self.dataset.attr_dataset.make_patches_of_image(name_img)

    progress = Progress(
        TextColumn(&#34;{task.fields[name]}&#34;, justify=&#34;right&#34;),
        BarColumn(bar_width=None),
        &#34;[progress.percentage]{task.percentage:&gt;3.1f}%&#34;,
        &#34;•&#34;,
        TimeElapsedColumn(),
        &#34;•&#34;,
        TimeRemainingColumn()
    ) # prepare the progress bar -&gt; tell which columns to display
    with progress: # progress bar manager to use the progress bar
        progression = progress.add_task(&#34;generation&#34;, name=&#34;[red]Progress&#34;, total=len(patches))
        for id,[input,output,filter] in enumerate(patches):
            if filter is True: # skip the image if the dataset ask to skip this patch (can be for multiple reasons -&gt; see DatasetFactory parameters supplied)
                continue
            input_adapted = np.stack((input,input,input),axis=0) # convert patch to rgb
            # pytorch can only make predictions for batches of images. That is why we create a &#34;batch&#34; of one image by adding one dimension to the image:
            #shape : (1, img_width, img_height, 3)
            input_adapted = input_adapted.reshape((1,*input_adapted.shape))
            # predict output with cpu if no device (gpu) is provided else predict with gpu and ransfer the result on cpu
            prediction = model(input) if device is None else model(torch.tensor(input_adapted).to(device)).cpu()
            # get the pixel position of the patch
            pos_x,pos_y = self.dataset.attr_patch_creator.get_position_patch(id,original_img.shape)
            if len(output) &gt; 3:
                raise Exception(f&#34;{len(output)} classes : Too much : cannot use this vizualization technic&#34;)
            input = normalize(input)
            resized_grid_size = self.dataset.attr_patch_creator.attr_grid_size_px
            input = np.stack((input,)*3,axis=-1) # convert in rgb. NB: (input,)*3 &lt;=&gt; (input,input,input)
            # initialize the overlay for the patch
            color_true = np.ones((resized_grid_size,resized_grid_size,3))
            color_pred = np.ones((resized_grid_size,resized_grid_size,3))
            for i,c in enumerate(output):
                color_true[:,:,i] *= c
            if device is not None:
                prediction = prediction.cpu().detach().numpy()
            for i,c in enumerate(prediction[0]):

                color_pred[:,:,i] *= c

            coordx1_not_resize = pos_x
            # coordx1 = int(transformation_matrix.dot(np.array([coordx1_not_resize,0,1]))[0])
            coordx2_not_resize = coordx1_not_resize + self.dataset.attr_patch_creator.attr_grid_size_px
            # coordx2 = int(transformation_matrix.dot(np.array([coordx2_not_resize,0,1]))[0])
            coordy1_not_resize = pos_y
            # coordy1 = int(transformation_matrix.dot(np.array([0,coordy1_not_resize,1]))[1])
            coordy2_not_resize = coordy1_not_resize + self.dataset.attr_patch_creator.attr_grid_size_px
            # coordy2 = int(transformation_matrix.dot(np.array([0,coordy2_not_resize,1]))[1])
            overlay_true[coordx1_not_resize:coordx2_not_resize,coordy1_not_resize:coordy2_not_resize,:] = input * (1-blending_factor) + color_true * blending_factor
            overlay_pred[coordx1_not_resize:coordx2_not_resize,coordy1_not_resize:coordy2_not_resize,:] = input * (1-blending_factor) + color_pred * blending_factor
            progress.update(progression, advance=1)
    return overlay_true,overlay_pred,original_img</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="main.src.analysis" href="index.html">main.src.analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="main.src.analysis.tools.RGB_Overlay_Patch" href="#main.src.analysis.tools.RGB_Overlay_Patch">RGB_Overlay_Patch</a></code></h4>
<ul class="">
<li><code><a title="main.src.analysis.tools.RGB_Overlay_Patch.call" href="#main.src.analysis.tools.RGB_Overlay_Patch.call">call</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>