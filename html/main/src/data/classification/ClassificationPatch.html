<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>main.src.data.classification.ClassificationPatch API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main.src.data.classification.ClassificationPatch</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import random
from typing import Optional
from typing import Tuple

import numpy as np
from rasterio.transform import Affine, rowcol

from main.src.data.Augmentation.Augmenters.Augmenter0 import Augmenter0
from main.src.data.Augmentation.Augmenters.Augmenter1 import Augmenter1
from main.src.data.Augmentation.Augmenters.NoAugmenter import NoAugmenter
from main.src.data.Augmentation.Augmenters.enums import EnumAugmenter
from main.src.data.TwoWayDict import Way
from main.src.data.balance_classes.balance_classes import BalanceClasses1
from main.src.data.balance_classes.enums import EnumBalance
from main.src.data.balance_classes.no_balance import NoBalance
from main.src.data.balance_classes.only_other import BalanceClasses2
from main.src.data.classification.LabelModifier.LabelModifier1 import LabelModifier1
from main.src.data.classification.LabelModifier.LabelModifier2 import LabelModifier2
from main.src.data.classification.LabelModifier.NoLabelModifier import NoLabelModifier
from main.src.data.classification.enums import EnumLabelModifier
from main.src.data.enums import EnumClasses
from main.src.data.patch_creator.patch_creator0 import Patch_creator0
from main.src.data.resizer import Resizer
from main.src.data.segmentation.DataSegmentation import DataSentinel1Segmentation

from main.src.data.segmentation.NumpyAnnotations import NumpyAnnotations
from main.src.data.segmentation.PointAnnotations import PointAnnotations


class ClassificationPatch(DataSentinel1Segmentation):
    &#34;&#34;&#34;Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches,
    filteer them.

    Args:
        patch_creator: the object of PatchCreator0 class managing patches
        input_size: the size of the image provided as input to the attr_model ⚠️
        limit_num_images: limit the number of image in the attr_dataset per epoch (before filtering)
        balance: EnumBalance indicating the class used to balance images
        augmentations_img: opt str, list of augmentations to apply separated by commas to apply to source image
        augmenter_img: opt EnumAugmenter, name of the augmenter to use on source image
        augmentations_patch: opt str, list of augmentations to apply separated by commas to apply to source image
        augmenter_patch: opt EnumAugmenter, name of the augmenter to use on patches
        augmentation_factor: the number of replicas of the original attr_dataset to do
        label_modifier: EnumLabelModifier
    &#34;&#34;&#34;

    def __init__(self, patch_creator: Patch_creator0, input_size: int = None,
                 limit_num_images: int = None, balance: EnumBalance = EnumBalance.NoBalance,
                 augmentations_img=&#34;none&#34;, augmenter_img: EnumAugmenter = EnumAugmenter.NoAugmenter,
                 augmentations_patch=&#34;none&#34;, augmenter_patch: EnumAugmenter = EnumAugmenter.NoAugmenter,
                 augmentation_factor: int = 100, label_modifier: EnumLabelModifier = EnumLabelModifier.NoLabelModifier,
                 classes_to_use: Tuple[EnumClasses] = (EnumClasses.Seep, EnumClasses.Spill),
                 tr_percent=0.7):
        self.attr_name = self.__class__.__name__  # save the name of the class used for reproductibility purposes
        self.patch_creator = patch_creator
        self.attr_limit_num_images = limit_num_images
        self.attr_resizer = Resizer(out_size_w=input_size)
        self.attr_augmentation_factor = augmentation_factor
        super(ClassificationPatch, self).__init__(limit_num_images, input_size=input_size,)
        self.tr_keys = list(self.images.keys())[:int(len(self.images) * tr_percent)]
        self.valid_keys = list(self.images.keys())[int(len(self.images) * tr_percent):]
        self.attr_global_name = &#34;attr_dataset&#34;
        if label_modifier == EnumLabelModifier.NoLabelModifier:
            self.attr_label_modifier = NoLabelModifier()
        elif label_modifier == EnumLabelModifier.LabelModifier1:
            self.attr_label_modifier = LabelModifier1(classes_to_use=classes_to_use,
                                                      original_class_mapping=DataSentinel1Segmentation.attr_original_class_mapping)
        elif label_modifier == EnumLabelModifier.LabelModifier2:
            self.attr_label_modifier = LabelModifier2(classes_to_use=classes_to_use,
                                                      original_class_mapping=DataSentinel1Segmentation.attr_original_class_mapping)
        else:
            raise NotImplementedError(f&#34;{label_modifier} is not implemented&#34;)
        if balance == EnumBalance.NoBalance:
            self.attr_balance = NoBalance()
        elif balance == EnumBalance.BalanceClasses1:
            # see class DataSentinel1Segmentation for documentation on attr_class_mapping storage and access to values
            self.attr_balance = BalanceClasses1(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        elif balance == EnumBalance.BalanceClasses2:
            self.attr_balance = BalanceClasses2(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        else:
            raise NotImplementedError
        if augmentations_img != &#34;none&#34;:
            if augmenter_img == EnumAugmenter.Augmenter0:
                self.attr_img_augmenter = Augmenter0(allowed_transformations=augmentations_img)
                self.generator = self.generate_item_step_by_step
                self.annotations_labels = NumpyAnnotations()
            elif augmenter_img == EnumAugmenter.Augmenter1:
                self.attr_img_augmenter = Augmenter1(allowed_transformations=augmentations_img,
                                                     patch_size_before_final_resize=
                                                     self.patch_creator.attr_grid_size_px,
                                                     patch_size_final_resize=input_size
                                                     )
                self.generator = self.generate_item_with_augmentation_at_once
                self.annotations_labels = PointAnnotations()

            else:
                self.generator = self.generate_item_step_by_step
                raise NotImplementedError(f&#34;{augmenter_img} is not implemented&#34;)
        else:
            self.generator = self.generate_item_step_by_step
            self.attr_img_augmenter = NoAugmenter()
        if augmentations_patch != &#34;none&#34;:
            if augmenter_patch == EnumAugmenter.Augmenter0:
                self.attr_patch_augmenter = Augmenter0(allowed_transformations=augmentations_patch)
            else:
                raise NotImplementedError(f&#34;{augmenter_patch} is not implemented&#34;)
        else:
            self.attr_patch_augmenter = NoAugmenter()
        # Cache to store between epochs rejected images if we have no image augmenter
        self.cache_img_id_rejected = []

    def get_geographic_coords_of_patch(self, name_src_img, patch_id):
        &#34;&#34;&#34;Get the coordinates of the upper left pixel of the patch specified

        Args:
            name_src_img: str, uniq id of the source image
            patch_id: int, id of the patch to get coordinates

        Returns:
            tuple xcoord,ycoord coordinates of the upper left pixel of the patch specified
        &#34;&#34;&#34;
        img = self.getimage(name_src_img)  # read image from the hdf5 file
        transform_array = self.images_infos[name_src_img][&#34;transform&#34;]  # get the corresponding transformation array
        transform_array = np.array(transform_array)
        # transfer it into a rasterio AffineTransformation object
        transform = Affine.from_gdal(a=transform_array[0, 0], b=transform_array[0, 1], c=transform_array[0, 2],
                                     d=transform_array[1, 0], e=transform_array[1, 1], f=transform_array[1, 2])
        # Get the position of the upperleft pixel on the global image
        posx, posy = self.patch_creator.get_position_patch(patch_id=patch_id, input_shape=img.shape)
        # Get the corresponding geographical coordinates
        return rowcol(transform, posx, posy)

    def __iter__(self, dataset=&#34;tr&#34;):
        return iter(self.generator(dataset))

    def generate_item_with_augmentation_at_once(self,dataset=&#34;tr&#34;):
        &#34;&#34;&#34;

        Args:

        Returns:
            generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
            tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
                   classif: np.ndarray (shape (num_classes,), classification patch ;
                   transformation_matrix:  the transformation matrix to transform the source image
                   item: str name of the source image
        &#34;&#34;&#34;
        if isinstance(self.attr_img_augmenter, Augmenter1) is False:
            raise Exception(&#34;Only augmenter1 is supported with this method of attr_dataset generation&#34;)
        if isinstance(self.attr_patch_augmenter, NoAugmenter) is False:
            raise Exception(&#34;The patch augmenter is not supported when you choose augmenter1 for image augmenter&#34;)
        images_available = self.tr_keys if dataset == &#34;tr&#34; else self.valid_keys
        for num_dataset in range(self.attr_augmentation_factor):
            random.shuffle(images_available)
            for item in images_available:
                image = self.images[item]
                partial_transformation_matrix = self.attr_img_augmenter.choose_new_augmentations(image)
                for patch_upper_left_corner_coords in np.random.permutation(
                        self.attr_img_augmenter.get_grid(image.shape, partial_transformation_matrix)):
                    annotations_patch,transformation_matrix = self.attr_img_augmenter.transform_label(self.annotations_labels.get,item,
                                                                                partial_transformation_matrix,patch_upper_left_corner_coords)
                    # Create the classification label with the proper technic ⚠️⚠️ inheritance
                    classification, balance_reject = self.make_classification_label(annotations_patch)  # ~ 2 ms
                    if balance_reject is True:
                        continue
                    image = np.array(image,dtype=np.float32)
                    image_patch, transformation_matrix = self.attr_img_augmenter.transform_image(
                        image=image,
                        partial_transformation_matrix=partial_transformation_matrix,
                        patch_upper_left_corner_coords=patch_upper_left_corner_coords
                        )
                    reject = self.patch_creator.check_reject(image_patch, threshold_px=10)
                    if reject is True:
                        continue
                    # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                    image_patch = np.stack((image_patch, image_patch, image_patch), axis=0)  # 0 ns most of the time
                    # yield image_patch, annotations, transformation_matrix, item
                    yield image_patch, classification, transformation_matrix, item

    def generate_item_step_by_step(self, dataset=&#34;tr&#34;):  # btwn 25 and 50 ms
        &#34;&#34;&#34;Magic method of python called by the object[id] syntax.

        get the patch of global int id id
        Args:
            dataset:

        Returns:
            generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
            tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
                   classif: np.ndarray (shape (num_classes,), classification patch ;
                   None:  no transformation matrix is available for this method
                   item: str name of the source image
        &#34;&#34;&#34;
        if isinstance(self.attr_img_augmenter, Augmenter1) is True:
            raise Exception(&#34;Augmenter1 is not supported with this method of attr_dataset generation. Use Augmenter0&#34;)
        images_available = self.tr_keys if dataset == &#34;tr&#34; else self.valid_keys
        # images_available = list(self.images.keys())
        for num_dataset in range(self.attr_augmentation_factor):
            random.shuffle(images_available)
            for item in images_available:
                image = self.images[item]
                annotations = self.annotations_labels[item]
                for patch_id in np.random.permutation(range(self.patch_creator.num_available_patches(image))):

                    if isinstance(self.attr_img_augmenter, NoAugmenter) and \
                            [item, patch_id] in self.cache_img_id_rejected:
                        continue  # to save computation time and keep np array output
                    # Make augmentations on input image if necessary (thanks to NoAugment class)
                    image, annotations = self.attr_img_augmenter.transform(image, annotations)
                    # get the patch with the selected id for the input image and the annotation
                    ## two lines: btwn 21 and 54 ms
                    img_patch, reject = self.patch_creator(image, item, patch_id=patch_id)  # btwn 10 ms and 50 ms
                    if reject is True:
                        continue  # to save computation time and keep np array output
                    annotations_patch, _ = self.patch_creator(annotations, item,
                                                              patch_id=patch_id)  # btwn 10 ms and 30 ms (10 ms most of the time)
                    # Make augmentations on patch if necessary (thanks to NoAugment class) and if it is not rejected
                    img_patch, annotations_patch = self.attr_patch_augmenter.transform(img_patch, annotations_patch)
                    # we reject an image if it contains margins (cf patchcreator)
                    # resize the image at the provided size in the constructor (with the magic method __call__ of the Resizer object
                    input = self.attr_resizer(img_patch)  # ~ 0 ns most of the time, 1 ms sometimes
                    # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                    input = np.stack((input, input, input), axis=0)  # 0 ns most of the time
                    input = (input - self.pixel_stats[&#34;mean&#34;]) / self.pixel_stats[&#34;std&#34;]
                    # Create the classification label with the proper technic ⚠️⚠️ inheritance
                    classif, balance_reject = self.make_classification_label(annotations_patch)  # ~ 2 ms
                    # As the balancing operation are done in the make_classification_label method, we reject an image
                    # if it is rejected due to margins or balancing
                    reject = reject or balance_reject
                    if isinstance(self.attr_img_augmenter, NoAugmenter) and reject is True:
                        self.cache_img_id_rejected.append([item, patch_id])
                        continue
                    yield input, classif, None, item

    def make_classification_label(self, annotations_patch):
        &#34;&#34;&#34;Creates the classification label based on the annotation patch image

        Indicates if we need to reject the patch due to overrepresented class

        Args:
            annotations_patch: np.ndarray 2d containing for each pixel the class of this pixel

        Returns: the classification label

        &#34;&#34;&#34;

        output = np.zeros((len(self.attr_original_class_mapping),), dtype=np.float32)  # 0 ns
        for value in self.attr_original_class_mapping.keys(Way.ORIGINAL_WAY):  # btwn 1 and 2 ms for the for loop
            # for each class of the original attr_dataset, we put a probability of presence of one if the class is in the patch
            value = int(value)
            #  if the class is in the patch
            if value in annotations_patch:
                output[value] = 1.
        # Check if we need to reject the patch due to overrepresented class
        balance_reject = self.attr_balance.filter(output)
        # Modify the label if require
        output = self.attr_label_modifier.make_classification_label(output)
        return output, balance_reject

    def __len__(self):
        return None

    def make_patches_of_image(self, name: str):
        &#34;&#34;&#34;Creates and returns all patches of an image

        Args:
            name: uniq str id of the image

        Returns:
            list of list of:

            - patch: np.ndarray
            - classif: np.ndarray classification label as returned by make_classification_label
            - reject: bool reject only based on margins
        &#34;&#34;&#34;
        last_image = np.copy(np.array(self.getimage(name), dtype=np.float32))
        liste_patches = []
        num_patches = self.patch_creator.num_available_patches(last_image)
        # Create all the patches of input images
        for id in range(num_patches):
            patch, reject = self.patch_creator(last_image, name, patch_id=id)
            liste_patches.append([patch])
            liste_patches[id].append(reject)
        annotations = np.array(self.annotations_labels[name], dtype=np.float32)
        for id in range(num_patches):
            patch, reject = self.patch_creator(annotations, name, patch_id=id)
            classif = self.make_classification_label(patch)
            # we ignore balancing rejects
            liste_patches[id].insert(1, classif[0])
        return liste_patches

    def len(self, dataset: str) -&gt; Optional[int]:
        return None
# Tests in DatasetFactory</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch"><code class="flex name class">
<span>class <span class="ident">ClassificationPatch</span></span>
<span>(</span><span>patch_creator: <a title="main.src.data.patch_creator.patch_creator0.Patch_creator0" href="../patch_creator/patch_creator0.html#main.src.data.patch_creator.patch_creator0.Patch_creator0">Patch_creator0</a>, input_size: int = None, limit_num_images: int = None, balance: <a title="main.src.data.balance_classes.enums.EnumBalance" href="../balance_classes/enums.html#main.src.data.balance_classes.enums.EnumBalance">EnumBalance</a> = EnumBalance.NoBalance, augmentations_img='none', augmenter_img: <a title="main.src.data.Augmentation.Augmenters.enums.EnumAugmenter" href="../Augmentation/Augmenters/enums.html#main.src.data.Augmentation.Augmenters.enums.EnumAugmenter">EnumAugmenter</a> = EnumAugmenter.NoAugmenter, augmentations_patch='none', augmenter_patch: <a title="main.src.data.Augmentation.Augmenters.enums.EnumAugmenter" href="../Augmentation/Augmenters/enums.html#main.src.data.Augmentation.Augmenters.enums.EnumAugmenter">EnumAugmenter</a> = EnumAugmenter.NoAugmenter, augmentation_factor: int = 100, label_modifier: <a title="main.src.data.classification.enums.EnumLabelModifier" href="enums.html#main.src.data.classification.enums.EnumLabelModifier">EnumLabelModifier</a> = EnumLabelModifier.NoLabelModifier, classes_to_use: Tuple[<a title="main.src.data.enums.EnumClasses" href="../enums.html#main.src.data.enums.EnumClasses">EnumClasses</a>] = (&lt;EnumClasses.Seep: &#x27;seep&#x27;&gt;, &lt;EnumClasses.Spill: &#x27;spill&#x27;&gt;), tr_percent=0.7)</span>
</code></dt>
<dd>
<div class="desc"><p>Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches,
filteer them.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>patch_creator</code></strong></dt>
<dd>the object of PatchCreator0 class managing patches</dd>
<dt><strong><code>input_size</code></strong></dt>
<dd>the size of the image provided as input to the attr_model ⚠️</dd>
<dt><strong><code>limit_num_images</code></strong></dt>
<dd>limit the number of image in the attr_dataset per epoch (before filtering)</dd>
<dt><strong><code>balance</code></strong></dt>
<dd>EnumBalance indicating the class used to balance images</dd>
<dt><strong><code>augmentations_img</code></strong></dt>
<dd>opt str, list of augmentations to apply separated by commas to apply to source image</dd>
<dt><strong><code>augmenter_img</code></strong></dt>
<dd>opt EnumAugmenter, name of the augmenter to use on source image</dd>
<dt><strong><code>augmentations_patch</code></strong></dt>
<dd>opt str, list of augmentations to apply separated by commas to apply to source image</dd>
<dt><strong><code>augmenter_patch</code></strong></dt>
<dd>opt EnumAugmenter, name of the augmenter to use on patches</dd>
<dt><strong><code>augmentation_factor</code></strong></dt>
<dd>the number of replicas of the original attr_dataset to do</dd>
<dt><strong><code>label_modifier</code></strong></dt>
<dd>EnumLabelModifier</dd>
</dl>
<p>Class giving access and managing the original attr_dataset stored in the hdf5 and json files</p>
<h2 id="args_1">Args</h2>
<dl>
<dt><strong><code>limit_num_images</code></strong></dt>
<dd>limit the number of image in the attr_dataset per epoch (before filtering)</dd>
<dt><strong><code>input_size</code></strong></dt>
<dd>the size of the image provided as input to the attr_model ⚠️</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClassificationPatch(DataSentinel1Segmentation):
    &#34;&#34;&#34;Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches,
    filteer them.

    Args:
        patch_creator: the object of PatchCreator0 class managing patches
        input_size: the size of the image provided as input to the attr_model ⚠️
        limit_num_images: limit the number of image in the attr_dataset per epoch (before filtering)
        balance: EnumBalance indicating the class used to balance images
        augmentations_img: opt str, list of augmentations to apply separated by commas to apply to source image
        augmenter_img: opt EnumAugmenter, name of the augmenter to use on source image
        augmentations_patch: opt str, list of augmentations to apply separated by commas to apply to source image
        augmenter_patch: opt EnumAugmenter, name of the augmenter to use on patches
        augmentation_factor: the number of replicas of the original attr_dataset to do
        label_modifier: EnumLabelModifier
    &#34;&#34;&#34;

    def __init__(self, patch_creator: Patch_creator0, input_size: int = None,
                 limit_num_images: int = None, balance: EnumBalance = EnumBalance.NoBalance,
                 augmentations_img=&#34;none&#34;, augmenter_img: EnumAugmenter = EnumAugmenter.NoAugmenter,
                 augmentations_patch=&#34;none&#34;, augmenter_patch: EnumAugmenter = EnumAugmenter.NoAugmenter,
                 augmentation_factor: int = 100, label_modifier: EnumLabelModifier = EnumLabelModifier.NoLabelModifier,
                 classes_to_use: Tuple[EnumClasses] = (EnumClasses.Seep, EnumClasses.Spill),
                 tr_percent=0.7):
        self.attr_name = self.__class__.__name__  # save the name of the class used for reproductibility purposes
        self.patch_creator = patch_creator
        self.attr_limit_num_images = limit_num_images
        self.attr_resizer = Resizer(out_size_w=input_size)
        self.attr_augmentation_factor = augmentation_factor
        super(ClassificationPatch, self).__init__(limit_num_images, input_size=input_size,)
        self.tr_keys = list(self.images.keys())[:int(len(self.images) * tr_percent)]
        self.valid_keys = list(self.images.keys())[int(len(self.images) * tr_percent):]
        self.attr_global_name = &#34;attr_dataset&#34;
        if label_modifier == EnumLabelModifier.NoLabelModifier:
            self.attr_label_modifier = NoLabelModifier()
        elif label_modifier == EnumLabelModifier.LabelModifier1:
            self.attr_label_modifier = LabelModifier1(classes_to_use=classes_to_use,
                                                      original_class_mapping=DataSentinel1Segmentation.attr_original_class_mapping)
        elif label_modifier == EnumLabelModifier.LabelModifier2:
            self.attr_label_modifier = LabelModifier2(classes_to_use=classes_to_use,
                                                      original_class_mapping=DataSentinel1Segmentation.attr_original_class_mapping)
        else:
            raise NotImplementedError(f&#34;{label_modifier} is not implemented&#34;)
        if balance == EnumBalance.NoBalance:
            self.attr_balance = NoBalance()
        elif balance == EnumBalance.BalanceClasses1:
            # see class DataSentinel1Segmentation for documentation on attr_class_mapping storage and access to values
            self.attr_balance = BalanceClasses1(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        elif balance == EnumBalance.BalanceClasses2:
            self.attr_balance = BalanceClasses2(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        else:
            raise NotImplementedError
        if augmentations_img != &#34;none&#34;:
            if augmenter_img == EnumAugmenter.Augmenter0:
                self.attr_img_augmenter = Augmenter0(allowed_transformations=augmentations_img)
                self.generator = self.generate_item_step_by_step
                self.annotations_labels = NumpyAnnotations()
            elif augmenter_img == EnumAugmenter.Augmenter1:
                self.attr_img_augmenter = Augmenter1(allowed_transformations=augmentations_img,
                                                     patch_size_before_final_resize=
                                                     self.patch_creator.attr_grid_size_px,
                                                     patch_size_final_resize=input_size
                                                     )
                self.generator = self.generate_item_with_augmentation_at_once
                self.annotations_labels = PointAnnotations()

            else:
                self.generator = self.generate_item_step_by_step
                raise NotImplementedError(f&#34;{augmenter_img} is not implemented&#34;)
        else:
            self.generator = self.generate_item_step_by_step
            self.attr_img_augmenter = NoAugmenter()
        if augmentations_patch != &#34;none&#34;:
            if augmenter_patch == EnumAugmenter.Augmenter0:
                self.attr_patch_augmenter = Augmenter0(allowed_transformations=augmentations_patch)
            else:
                raise NotImplementedError(f&#34;{augmenter_patch} is not implemented&#34;)
        else:
            self.attr_patch_augmenter = NoAugmenter()
        # Cache to store between epochs rejected images if we have no image augmenter
        self.cache_img_id_rejected = []

    def get_geographic_coords_of_patch(self, name_src_img, patch_id):
        &#34;&#34;&#34;Get the coordinates of the upper left pixel of the patch specified

        Args:
            name_src_img: str, uniq id of the source image
            patch_id: int, id of the patch to get coordinates

        Returns:
            tuple xcoord,ycoord coordinates of the upper left pixel of the patch specified
        &#34;&#34;&#34;
        img = self.getimage(name_src_img)  # read image from the hdf5 file
        transform_array = self.images_infos[name_src_img][&#34;transform&#34;]  # get the corresponding transformation array
        transform_array = np.array(transform_array)
        # transfer it into a rasterio AffineTransformation object
        transform = Affine.from_gdal(a=transform_array[0, 0], b=transform_array[0, 1], c=transform_array[0, 2],
                                     d=transform_array[1, 0], e=transform_array[1, 1], f=transform_array[1, 2])
        # Get the position of the upperleft pixel on the global image
        posx, posy = self.patch_creator.get_position_patch(patch_id=patch_id, input_shape=img.shape)
        # Get the corresponding geographical coordinates
        return rowcol(transform, posx, posy)

    def __iter__(self, dataset=&#34;tr&#34;):
        return iter(self.generator(dataset))

    def generate_item_with_augmentation_at_once(self,dataset=&#34;tr&#34;):
        &#34;&#34;&#34;

        Args:

        Returns:
            generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
            tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
                   classif: np.ndarray (shape (num_classes,), classification patch ;
                   transformation_matrix:  the transformation matrix to transform the source image
                   item: str name of the source image
        &#34;&#34;&#34;
        if isinstance(self.attr_img_augmenter, Augmenter1) is False:
            raise Exception(&#34;Only augmenter1 is supported with this method of attr_dataset generation&#34;)
        if isinstance(self.attr_patch_augmenter, NoAugmenter) is False:
            raise Exception(&#34;The patch augmenter is not supported when you choose augmenter1 for image augmenter&#34;)
        images_available = self.tr_keys if dataset == &#34;tr&#34; else self.valid_keys
        for num_dataset in range(self.attr_augmentation_factor):
            random.shuffle(images_available)
            for item in images_available:
                image = self.images[item]
                partial_transformation_matrix = self.attr_img_augmenter.choose_new_augmentations(image)
                for patch_upper_left_corner_coords in np.random.permutation(
                        self.attr_img_augmenter.get_grid(image.shape, partial_transformation_matrix)):
                    annotations_patch,transformation_matrix = self.attr_img_augmenter.transform_label(self.annotations_labels.get,item,
                                                                                partial_transformation_matrix,patch_upper_left_corner_coords)
                    # Create the classification label with the proper technic ⚠️⚠️ inheritance
                    classification, balance_reject = self.make_classification_label(annotations_patch)  # ~ 2 ms
                    if balance_reject is True:
                        continue
                    image = np.array(image,dtype=np.float32)
                    image_patch, transformation_matrix = self.attr_img_augmenter.transform_image(
                        image=image,
                        partial_transformation_matrix=partial_transformation_matrix,
                        patch_upper_left_corner_coords=patch_upper_left_corner_coords
                        )
                    reject = self.patch_creator.check_reject(image_patch, threshold_px=10)
                    if reject is True:
                        continue
                    # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                    image_patch = np.stack((image_patch, image_patch, image_patch), axis=0)  # 0 ns most of the time
                    # yield image_patch, annotations, transformation_matrix, item
                    yield image_patch, classification, transformation_matrix, item

    def generate_item_step_by_step(self, dataset=&#34;tr&#34;):  # btwn 25 and 50 ms
        &#34;&#34;&#34;Magic method of python called by the object[id] syntax.

        get the patch of global int id id
        Args:
            dataset:

        Returns:
            generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
            tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
                   classif: np.ndarray (shape (num_classes,), classification patch ;
                   None:  no transformation matrix is available for this method
                   item: str name of the source image
        &#34;&#34;&#34;
        if isinstance(self.attr_img_augmenter, Augmenter1) is True:
            raise Exception(&#34;Augmenter1 is not supported with this method of attr_dataset generation. Use Augmenter0&#34;)
        images_available = self.tr_keys if dataset == &#34;tr&#34; else self.valid_keys
        # images_available = list(self.images.keys())
        for num_dataset in range(self.attr_augmentation_factor):
            random.shuffle(images_available)
            for item in images_available:
                image = self.images[item]
                annotations = self.annotations_labels[item]
                for patch_id in np.random.permutation(range(self.patch_creator.num_available_patches(image))):

                    if isinstance(self.attr_img_augmenter, NoAugmenter) and \
                            [item, patch_id] in self.cache_img_id_rejected:
                        continue  # to save computation time and keep np array output
                    # Make augmentations on input image if necessary (thanks to NoAugment class)
                    image, annotations = self.attr_img_augmenter.transform(image, annotations)
                    # get the patch with the selected id for the input image and the annotation
                    ## two lines: btwn 21 and 54 ms
                    img_patch, reject = self.patch_creator(image, item, patch_id=patch_id)  # btwn 10 ms and 50 ms
                    if reject is True:
                        continue  # to save computation time and keep np array output
                    annotations_patch, _ = self.patch_creator(annotations, item,
                                                              patch_id=patch_id)  # btwn 10 ms and 30 ms (10 ms most of the time)
                    # Make augmentations on patch if necessary (thanks to NoAugment class) and if it is not rejected
                    img_patch, annotations_patch = self.attr_patch_augmenter.transform(img_patch, annotations_patch)
                    # we reject an image if it contains margins (cf patchcreator)
                    # resize the image at the provided size in the constructor (with the magic method __call__ of the Resizer object
                    input = self.attr_resizer(img_patch)  # ~ 0 ns most of the time, 1 ms sometimes
                    # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                    input = np.stack((input, input, input), axis=0)  # 0 ns most of the time
                    input = (input - self.pixel_stats[&#34;mean&#34;]) / self.pixel_stats[&#34;std&#34;]
                    # Create the classification label with the proper technic ⚠️⚠️ inheritance
                    classif, balance_reject = self.make_classification_label(annotations_patch)  # ~ 2 ms
                    # As the balancing operation are done in the make_classification_label method, we reject an image
                    # if it is rejected due to margins or balancing
                    reject = reject or balance_reject
                    if isinstance(self.attr_img_augmenter, NoAugmenter) and reject is True:
                        self.cache_img_id_rejected.append([item, patch_id])
                        continue
                    yield input, classif, None, item

    def make_classification_label(self, annotations_patch):
        &#34;&#34;&#34;Creates the classification label based on the annotation patch image

        Indicates if we need to reject the patch due to overrepresented class

        Args:
            annotations_patch: np.ndarray 2d containing for each pixel the class of this pixel

        Returns: the classification label

        &#34;&#34;&#34;

        output = np.zeros((len(self.attr_original_class_mapping),), dtype=np.float32)  # 0 ns
        for value in self.attr_original_class_mapping.keys(Way.ORIGINAL_WAY):  # btwn 1 and 2 ms for the for loop
            # for each class of the original attr_dataset, we put a probability of presence of one if the class is in the patch
            value = int(value)
            #  if the class is in the patch
            if value in annotations_patch:
                output[value] = 1.
        # Check if we need to reject the patch due to overrepresented class
        balance_reject = self.attr_balance.filter(output)
        # Modify the label if require
        output = self.attr_label_modifier.make_classification_label(output)
        return output, balance_reject

    def __len__(self):
        return None

    def make_patches_of_image(self, name: str):
        &#34;&#34;&#34;Creates and returns all patches of an image

        Args:
            name: uniq str id of the image

        Returns:
            list of list of:

            - patch: np.ndarray
            - classif: np.ndarray classification label as returned by make_classification_label
            - reject: bool reject only based on margins
        &#34;&#34;&#34;
        last_image = np.copy(np.array(self.getimage(name), dtype=np.float32))
        liste_patches = []
        num_patches = self.patch_creator.num_available_patches(last_image)
        # Create all the patches of input images
        for id in range(num_patches):
            patch, reject = self.patch_creator(last_image, name, patch_id=id)
            liste_patches.append([patch])
            liste_patches[id].append(reject)
        annotations = np.array(self.annotations_labels[name], dtype=np.float32)
        for id in range(num_patches):
            patch, reject = self.patch_creator(annotations, name, patch_id=id)
            classif = self.make_classification_label(patch)
            # we ignore balancing rejects
            liste_patches[id].insert(1, classif[0])
        return liste_patches

    def len(self, dataset: str) -&gt; Optional[int]:
        return None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation" href="../segmentation/DataSegmentation.html#main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation">DataSentinel1Segmentation</a></li>
<li><a title="main.src.param_savers.BaseClass.BaseClass" href="../../param_savers/BaseClass.html#main.src.param_savers.BaseClass.BaseClass">BaseClass</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.generate_item_step_by_step"><code class="name flex">
<span>def <span class="ident">generate_item_step_by_step</span></span>(<span>self, dataset='tr')</span>
</code></dt>
<dd>
<div class="desc"><p>Magic method of python called by the object[id] syntax.</p>
<p>get the patch of global int id id</p>
<h2 id="args">Args</h2>
<p>dataset:</p>
<h2 id="returns">Returns</h2>
<dl>
<dt>generator of the attr_dataset (object that support <strong>iter</strong> and <strong>next</strong> magic methods)</dt>
<dt><code>tuple</code></dt>
<dd>input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
classif: np.ndarray (shape (num_classes,), classification patch ;
None:
no transformation matrix is available for this method
item: str name of the source image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_item_step_by_step(self, dataset=&#34;tr&#34;):  # btwn 25 and 50 ms
    &#34;&#34;&#34;Magic method of python called by the object[id] syntax.

    get the patch of global int id id
    Args:
        dataset:

    Returns:
        generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
        tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
               classif: np.ndarray (shape (num_classes,), classification patch ;
               None:  no transformation matrix is available for this method
               item: str name of the source image
    &#34;&#34;&#34;
    if isinstance(self.attr_img_augmenter, Augmenter1) is True:
        raise Exception(&#34;Augmenter1 is not supported with this method of attr_dataset generation. Use Augmenter0&#34;)
    images_available = self.tr_keys if dataset == &#34;tr&#34; else self.valid_keys
    # images_available = list(self.images.keys())
    for num_dataset in range(self.attr_augmentation_factor):
        random.shuffle(images_available)
        for item in images_available:
            image = self.images[item]
            annotations = self.annotations_labels[item]
            for patch_id in np.random.permutation(range(self.patch_creator.num_available_patches(image))):

                if isinstance(self.attr_img_augmenter, NoAugmenter) and \
                        [item, patch_id] in self.cache_img_id_rejected:
                    continue  # to save computation time and keep np array output
                # Make augmentations on input image if necessary (thanks to NoAugment class)
                image, annotations = self.attr_img_augmenter.transform(image, annotations)
                # get the patch with the selected id for the input image and the annotation
                ## two lines: btwn 21 and 54 ms
                img_patch, reject = self.patch_creator(image, item, patch_id=patch_id)  # btwn 10 ms and 50 ms
                if reject is True:
                    continue  # to save computation time and keep np array output
                annotations_patch, _ = self.patch_creator(annotations, item,
                                                          patch_id=patch_id)  # btwn 10 ms and 30 ms (10 ms most of the time)
                # Make augmentations on patch if necessary (thanks to NoAugment class) and if it is not rejected
                img_patch, annotations_patch = self.attr_patch_augmenter.transform(img_patch, annotations_patch)
                # we reject an image if it contains margins (cf patchcreator)
                # resize the image at the provided size in the constructor (with the magic method __call__ of the Resizer object
                input = self.attr_resizer(img_patch)  # ~ 0 ns most of the time, 1 ms sometimes
                # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                input = np.stack((input, input, input), axis=0)  # 0 ns most of the time
                input = (input - self.pixel_stats[&#34;mean&#34;]) / self.pixel_stats[&#34;std&#34;]
                # Create the classification label with the proper technic ⚠️⚠️ inheritance
                classif, balance_reject = self.make_classification_label(annotations_patch)  # ~ 2 ms
                # As the balancing operation are done in the make_classification_label method, we reject an image
                # if it is rejected due to margins or balancing
                reject = reject or balance_reject
                if isinstance(self.attr_img_augmenter, NoAugmenter) and reject is True:
                    self.cache_img_id_rejected.append([item, patch_id])
                    continue
                yield input, classif, None, item</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.generate_item_with_augmentation_at_once"><code class="name flex">
<span>def <span class="ident">generate_item_with_augmentation_at_once</span></span>(<span>self, dataset='tr')</span>
</code></dt>
<dd>
<div class="desc"><p>Args:</p>
<h2 id="returns">Returns</h2>
<dl>
<dt>generator of the attr_dataset (object that support <strong>iter</strong> and <strong>next</strong> magic methods)</dt>
<dt><code>tuple</code></dt>
<dd>input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
classif: np.ndarray (shape (num_classes,), classification patch ;
transformation_matrix:
the transformation matrix to transform the source image
item: str name of the source image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_item_with_augmentation_at_once(self,dataset=&#34;tr&#34;):
    &#34;&#34;&#34;

    Args:

    Returns:
        generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
        tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
               classif: np.ndarray (shape (num_classes,), classification patch ;
               transformation_matrix:  the transformation matrix to transform the source image
               item: str name of the source image
    &#34;&#34;&#34;
    if isinstance(self.attr_img_augmenter, Augmenter1) is False:
        raise Exception(&#34;Only augmenter1 is supported with this method of attr_dataset generation&#34;)
    if isinstance(self.attr_patch_augmenter, NoAugmenter) is False:
        raise Exception(&#34;The patch augmenter is not supported when you choose augmenter1 for image augmenter&#34;)
    images_available = self.tr_keys if dataset == &#34;tr&#34; else self.valid_keys
    for num_dataset in range(self.attr_augmentation_factor):
        random.shuffle(images_available)
        for item in images_available:
            image = self.images[item]
            partial_transformation_matrix = self.attr_img_augmenter.choose_new_augmentations(image)
            for patch_upper_left_corner_coords in np.random.permutation(
                    self.attr_img_augmenter.get_grid(image.shape, partial_transformation_matrix)):
                annotations_patch,transformation_matrix = self.attr_img_augmenter.transform_label(self.annotations_labels.get,item,
                                                                            partial_transformation_matrix,patch_upper_left_corner_coords)
                # Create the classification label with the proper technic ⚠️⚠️ inheritance
                classification, balance_reject = self.make_classification_label(annotations_patch)  # ~ 2 ms
                if balance_reject is True:
                    continue
                image = np.array(image,dtype=np.float32)
                image_patch, transformation_matrix = self.attr_img_augmenter.transform_image(
                    image=image,
                    partial_transformation_matrix=partial_transformation_matrix,
                    patch_upper_left_corner_coords=patch_upper_left_corner_coords
                    )
                reject = self.patch_creator.check_reject(image_patch, threshold_px=10)
                if reject is True:
                    continue
                # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                image_patch = np.stack((image_patch, image_patch, image_patch), axis=0)  # 0 ns most of the time
                # yield image_patch, annotations, transformation_matrix, item
                yield image_patch, classification, transformation_matrix, item</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.get_geographic_coords_of_patch"><code class="name flex">
<span>def <span class="ident">get_geographic_coords_of_patch</span></span>(<span>self, name_src_img, patch_id)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the coordinates of the upper left pixel of the patch specified</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name_src_img</code></strong></dt>
<dd>str, uniq id of the source image</dd>
<dt><strong><code>patch_id</code></strong></dt>
<dd>int, id of the patch to get coordinates</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tuple xcoord,ycoord coordinates of the upper left pixel of the patch specified</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_geographic_coords_of_patch(self, name_src_img, patch_id):
    &#34;&#34;&#34;Get the coordinates of the upper left pixel of the patch specified

    Args:
        name_src_img: str, uniq id of the source image
        patch_id: int, id of the patch to get coordinates

    Returns:
        tuple xcoord,ycoord coordinates of the upper left pixel of the patch specified
    &#34;&#34;&#34;
    img = self.getimage(name_src_img)  # read image from the hdf5 file
    transform_array = self.images_infos[name_src_img][&#34;transform&#34;]  # get the corresponding transformation array
    transform_array = np.array(transform_array)
    # transfer it into a rasterio AffineTransformation object
    transform = Affine.from_gdal(a=transform_array[0, 0], b=transform_array[0, 1], c=transform_array[0, 2],
                                 d=transform_array[1, 0], e=transform_array[1, 1], f=transform_array[1, 2])
    # Get the position of the upperleft pixel on the global image
    posx, posy = self.patch_creator.get_position_patch(patch_id=patch_id, input_shape=img.shape)
    # Get the corresponding geographical coordinates
    return rowcol(transform, posx, posy)</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.len"><code class="name flex">
<span>def <span class="ident">len</span></span>(<span>self, dataset: str) ‑> Union[int, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def len(self, dataset: str) -&gt; Optional[int]:
    return None</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.make_classification_label"><code class="name flex">
<span>def <span class="ident">make_classification_label</span></span>(<span>self, annotations_patch)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates the classification label based on the annotation patch image</p>
<p>Indicates if we need to reject the patch due to overrepresented class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>annotations_patch</code></strong></dt>
<dd>np.ndarray 2d containing for each pixel the class of this pixel</dd>
</dl>
<p>Returns: the classification label</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_classification_label(self, annotations_patch):
    &#34;&#34;&#34;Creates the classification label based on the annotation patch image

    Indicates if we need to reject the patch due to overrepresented class

    Args:
        annotations_patch: np.ndarray 2d containing for each pixel the class of this pixel

    Returns: the classification label

    &#34;&#34;&#34;

    output = np.zeros((len(self.attr_original_class_mapping),), dtype=np.float32)  # 0 ns
    for value in self.attr_original_class_mapping.keys(Way.ORIGINAL_WAY):  # btwn 1 and 2 ms for the for loop
        # for each class of the original attr_dataset, we put a probability of presence of one if the class is in the patch
        value = int(value)
        #  if the class is in the patch
        if value in annotations_patch:
            output[value] = 1.
    # Check if we need to reject the patch due to overrepresented class
    balance_reject = self.attr_balance.filter(output)
    # Modify the label if require
    output = self.attr_label_modifier.make_classification_label(output)
    return output, balance_reject</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.make_patches_of_image"><code class="name flex">
<span>def <span class="ident">make_patches_of_image</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates and returns all patches of an image</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>uniq str id of the image</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>list of list of:</p>
<ul>
<li>patch: np.ndarray</li>
<li>classif: np.ndarray classification label as returned by make_classification_label</li>
<li>reject: bool reject only based on margins</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_patches_of_image(self, name: str):
    &#34;&#34;&#34;Creates and returns all patches of an image

    Args:
        name: uniq str id of the image

    Returns:
        list of list of:

        - patch: np.ndarray
        - classif: np.ndarray classification label as returned by make_classification_label
        - reject: bool reject only based on margins
    &#34;&#34;&#34;
    last_image = np.copy(np.array(self.getimage(name), dtype=np.float32))
    liste_patches = []
    num_patches = self.patch_creator.num_available_patches(last_image)
    # Create all the patches of input images
    for id in range(num_patches):
        patch, reject = self.patch_creator(last_image, name, patch_id=id)
        liste_patches.append([patch])
        liste_patches[id].append(reject)
    annotations = np.array(self.annotations_labels[name], dtype=np.float32)
    for id in range(num_patches):
        patch, reject = self.patch_creator(annotations, name, patch_id=id)
        classif = self.make_classification_label(patch)
        # we ignore balancing rejects
        liste_patches[id].insert(1, classif[0])
    return liste_patches</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation" href="../segmentation/DataSegmentation.html#main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation">DataSentinel1Segmentation</a></b></code>:
<ul class="hlist">
<li><code><a title="main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation.close" href="../segmentation/DataSegmentation.html#main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation.close">close</a></code></li>
<li><code><a title="main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation.get_all_items" href="../segmentation/DataSegmentation.html#main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation.get_all_items">get_all_items</a></code></li>
<li><code><a title="main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation.getitem" href="../segmentation/DataSegmentation.html#main.src.data.segmentation.DataSegmentation.DataSentinel1Segmentation.getitem">getitem</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="main.src.data.classification" href="index.html">main.src.data.classification</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch">ClassificationPatch</a></code></h4>
<ul class="">
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.generate_item_step_by_step" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.generate_item_step_by_step">generate_item_step_by_step</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.generate_item_with_augmentation_at_once" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.generate_item_with_augmentation_at_once">generate_item_with_augmentation_at_once</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.get_geographic_coords_of_patch" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.get_geographic_coords_of_patch">get_geographic_coords_of_patch</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.len" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.len">len</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.make_classification_label" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.make_classification_label">make_classification_label</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.make_patches_of_image" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.make_patches_of_image">make_patches_of_image</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>