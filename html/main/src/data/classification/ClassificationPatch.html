<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>main.src.data.classification.ClassificationPatch API documentation</title>
<meta name="description" content="Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches and
filter them" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main.src.data.classification.ClassificationPatch</code></h1>
</header>
<section id="section-intro">
<p>Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches and
filter them</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches and
filter them&#34;&#34;&#34;
import json
import random
from typing import Optional, List, Union, Dict
from typing import Tuple

import numpy as np
from h5py import File

from main.FolderInfos import FolderInfos
from main.src.data.Augmentation.Augmenters.Augmenter1 import Augmenter1
from main.src.data.Augmentation.Augmenters.NoAugmenter import NoAugmenter
from main.src.data.Datasets.AbstractDataset import AbstractDataset
from main.src.data.Datasets.Annotations.FabricPreprocessedCache import FabricPreprocessedCache
from main.src.data.Datasets.ImageDataset import ImageDataset
from main.src.data.TwoWayDict import TwoWayDict
from main.src.enums import EnumAugmenter
from main.src.data.balance_classes.BalanceClasses1 import BalanceClasses1
from main.src.data.balance_classes.BalanceClasses2 import BalanceClasses2
from main.src.enums import EnumBalance
from main.src.data.balance_classes.no_balance import NoBalance
from main.src.data.LabelModifier.LabelModifier0 import LabelModifier0
from main.src.data.LabelModifier.LabelModifier1 import LabelModifier1
from main.src.data.LabelModifier.LabelModifier2 import LabelModifier2
from main.src.data.Standardizer.AbstractStandardizer import AbstractStandardizer
from main.src.enums import EnumLabelModifier
from main.src.enums import EnumClasses
from main.src.data.MarginCheck import MarginCheck
from main.src.data.Datasets import PointDataset
from main.src.enums import EnumDataset
from main.src.param_savers.BaseClass import BaseClass


class ClassificationPatch(BaseClass):
    &#34;&#34;&#34;Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches and
    filter them.

    Args:
        patch_creator: the object of PatchCreator0 class managing patches
        input_size: the size of the image provided as input to the attr_model ⚠️
        limit_num_images: limit the number of image in the attr_dataset per epoch (before filtering)
        balance: EnumBalance indicating the class used to balance images
        augmentations_img: opt str, list of augmentations to apply separated by commas to apply to source image
        augmenter_img: opt EnumAugmenter, name of the augmenter to use on source image
        augmentation_factor: the number of replicas of the original attr_dataset to do
        label_modifier: EnumLabelModifier
    &#34;&#34;&#34;
    def __init__(self, input_size: int = None,
                 limit_num_images: int = None, balance: EnumBalance = EnumBalance.NoBalance,
                 augmentations_img=&#34;none&#34;, augmenter_img: EnumAugmenter = EnumAugmenter.NoAugmenter,
                 augmentation_factor: int = 100, label_modifier: EnumLabelModifier = EnumLabelModifier.NoLabelModifier,
                 classes_to_use: Tuple[EnumClasses] = (EnumClasses.Seep, EnumClasses.Spill),
                 tr_percent=0.7, grid_size_px: int = 1000, threshold_margin:int = 1000):
        self.attr_name = self.__class__.__name__  # save the name of the class used for reproductibility purposes
        self.attr_global_name = &#34;attr_dataset&#34;
        self.attr_image_dataset, self.attr_label_dataset,self.dico_infos = FabricPreprocessedCache()()
        self.attr_grid_size_px = grid_size_px
        self.attr_limit_num_images = limit_num_images
        self.attr_check_margin_reject = MarginCheck(threshold=threshold_margin)
        self.attr_augmentation_factor = augmentation_factor
        with self.attr_image_dataset as images:
            self.datasets = {
                &#34;tr&#34;:list(images.keys())[:int(len(images) * tr_percent)],
                &#34;valid&#34;:list(images.keys())[int(len(images) * tr_percent):]
            }
        self.attr_global_name = &#34;attr_dataset&#34;
        if label_modifier == EnumLabelModifier.NoLabelModifier:
            self.attr_label_modifier = LabelModifier0(class_mapping=ClassificationPatch.attr_original_class_mapping)
        elif label_modifier == EnumLabelModifier.LabelModifier1:
            self.attr_label_modifier = LabelModifier1(classes_to_use=classes_to_use,
                                                      original_class_mapping=ClassificationPatch.attr_original_class_mapping)
        elif label_modifier == EnumLabelModifier.LabelModifier2:
            self.attr_label_modifier = LabelModifier2(classes_to_use=classes_to_use,
                                                      original_class_mapping=ClassificationPatch.attr_original_class_mapping)
        else:
            raise NotImplementedError(f&#34;{label_modifier} is not implemented&#34;)
        if balance == EnumBalance.NoBalance:
            self.attr_balance = NoBalance()
        elif balance == EnumBalance.BalanceClasses1:
            # see class DataSentinel1Segmentation for documentation on attr_class_mapping storage and access to values
            self.attr_balance = BalanceClasses1(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        elif balance == EnumBalance.BalanceClasses2:
            self.attr_balance = BalanceClasses2(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        else:
            raise NotImplementedError
        if augmentations_img != &#34;none&#34;:
            if augmenter_img == EnumAugmenter.Augmenter1:
                self.attr_augmenter = Augmenter1(allowed_transformations=augmentations_img,
                                                 patch_size_before_final_resize=
                                                     self.attr_grid_size_px,
                                                 patch_size_final_resize=input_size
                                                 )

            else:
                self.attr_augmenter = NoAugmenter(allowed_transformations=augmentations_img,
                                                  patch_size_before_final_resize=
                                                     self.attr_grid_size_px,
                                                  patch_size_final_resize=input_size
                                                  )
        else:
            raise NotImplementedError(f&#34;{augmenter_img} is not implemented&#34;)
        # Cache to store between epochs rejected images if we have no image augmenter
        self.cache_img_id_rejected = []
    def set_datasets(self,image_dataset: ImageDataset, label_dataset: AbstractDataset, dico_infos: Dict):
        &#34;&#34;&#34;Change the origin of the patches

        Args:
            image_dataset: ImageDataset
            label_dataset: AbstractDataset Points or Images
            dico_infos: Dict, containing for each id of image the source image (under key source_img) and the transformation matrix (under key transformation_matrix) applied to get the patch
        Returns:

        &#34;&#34;&#34;
        self.attr_image_dataset = image_dataset
        self.attr_label_dataset = label_dataset
        self.dico_infos = dico_infos
    def __iter__(self, dataset: Union[EnumDataset,List[str]] = EnumDataset.Train):
        if isinstance(dataset,list):
            keys = dataset
        else:
            keys = self.datasets[dataset]
        return iter(self.generator(keys))

    def generator(self, images_available):
        &#34;&#34;&#34;

        Args:

        Returns:
            generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
            tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
                   classif: np.ndarray (shape (num_classes,), classification patch ;
                   transformation_matrix:  the transformation matrix to transform the source image
                   item: str name of the source image
        &#34;&#34;&#34;
        with self.attr_image_dataset as images_dataset:
            with self.attr_label_dataset as labels_dataset:
                for num_dataset in range(self.attr_augmentation_factor):
                    random.shuffle(images_available)
                    for item in images_available:
                        image = images_dataset.get(item)
                        image = np.array(image, dtype=np.float32)
                        polygons = labels_dataset.get(item)
                        partial_transformation_matrix = self.attr_augmenter.choose_new_augmentations(image)
                        for patch_upper_left_corner_coords in np.random.permutation(self.attr_augmenter.get_grid(image.shape, partial_transformation_matrix)):
                            annotations_patch, transformation_matrix = self.attr_augmenter.transform_label(
                                polygons=polygons,
                                partial_transformation_matrix=partial_transformation_matrix,
                                patch_upper_left_corner_coords=patch_upper_left_corner_coords
                            )
                            # Create the classification label with the proper technic
                            classification = self.attr_label_modifier.make_classification_label(annotations_patch)
                            balance_reject = self.attr_balance.filter(self.attr_label_modifier.get_initial_label())
                            if balance_reject is True:
                                continue
                            image_patch, transformation_matrix = self.attr_augmenter.transform_image(
                                image=image,
                                partial_transformation_matrix=partial_transformation_matrix,
                                patch_upper_left_corner_coords=patch_upper_left_corner_coords
                            )
                            reject = self.attr_check_margin_reject.check_reject(image_patch)
                            if reject is True:
                                continue
                            # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                            image_patch = np.stack((image_patch,)*3, axis=0)
                            yield image_patch, classification, transformation_matrix, item

    def get_patch(self,image: np.ndarray,annotation: np.ndarray, patch_upper_left_corner_coords: Tuple[int,int],
                  standardizer: AbstractStandardizer,label_encoding: TwoWayDict, transformation_matrix: Optional[np.ndarray] = None
                  ) -&gt; Tuple[np.ndarray,np.ndarray,np.ndarray]:
        &#34;&#34;&#34;Generate image patch and corresponding annotation for the given parameters

        Args:
            image: np.ndarray, image on which to apply the transformations
            annotation: np.ndarray, annotation on which to apply the transformations
            patch_upper_left_corner_coords: coordinates of the upper left corner to get
            standardizer: object giving allowing to standardize the patch
            label_encoding: TwoWayDict mapping between labels names and encoding as uint8
            transformation_matrix: Optional[np.ndarray] (3,3) transformation matrix to apply to the image and annotation ⚠️⚠️ no rotation allowed as we have an array annotation

        Returns:
            Tuple[np.ndarray,np.ndarray,np.ndarray]
            - image_patch: patch extracted from the image
            - classification: vector containing true probabilities of presence of annotation
            - transformation_matrix: 3,3 transformation matrix applied
        &#34;&#34;&#34;
        if transformation_matrix is None:
            transformation_matrix = np.identity(3)
        image_patch, transformation_matrix = self.attr_augmenter.transform_image(
            image=image,
            partial_transformation_matrix=transformation_matrix,
            patch_upper_left_corner_coords=patch_upper_left_corner_coords
        )
        annotation_patch, transformation_matrix = self.attr_augmenter.transform_image(
            image=annotation,
            partial_transformation_matrix=transformation_matrix,
            patch_upper_left_corner_coords=patch_upper_left_corner_coords
        )
        classification = self.attr_label_modifier.make_classification_label(annotation_patch)
        image_patch = np.stack((image_patch,)*3, axis=0)
        image_patch = standardizer.standardize(image_patch)
        return image_patch,classification,transformation_matrix
    def __len__(self):
        return None
    def set_standardizer(self, standardizer: AbstractStandardizer):
        self.attr_standardizer = standardizer
    def set_annotator(self,annotations):
        self.annotations_labels = annotations

    def len(self, dataset: str) -&gt; Optional[int]:
        return None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch"><code class="flex name class">
<span>class <span class="ident">ClassificationPatch</span></span>
<span>(</span><span>input_size: int = None, limit_num_images: int = None, balance: <a title="main.src.enums.EnumBalance" href="../../enums.html#main.src.enums.EnumBalance">EnumBalance</a> = EnumBalance.NoBalance, augmentations_img='none', augmenter_img: <a title="main.src.enums.EnumAugmenter" href="../../enums.html#main.src.enums.EnumAugmenter">EnumAugmenter</a> = EnumAugmenter.NoAugmenter, augmentation_factor: int = 100, label_modifier: <a title="main.src.enums.EnumLabelModifier" href="../../enums.html#main.src.enums.EnumLabelModifier">EnumLabelModifier</a> = EnumLabelModifier.NoLabelModifier, classes_to_use: Tuple[<a title="main.src.enums.EnumClasses" href="../../enums.html#main.src.enums.EnumClasses">EnumClasses</a>] = (&lt;EnumClasses.Seep: &#x27;seep&#x27;&gt;, &lt;EnumClasses.Spill: &#x27;spill&#x27;&gt;), tr_percent=0.7, grid_size_px: int = 1000, threshold_margin: int = 1000)</span>
</code></dt>
<dd>
<div class="desc"><p>Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches and
filter them.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>patch_creator</code></strong></dt>
<dd>the object of PatchCreator0 class managing patches</dd>
<dt><strong><code>input_size</code></strong></dt>
<dd>the size of the image provided as input to the attr_model ⚠️</dd>
<dt><strong><code>limit_num_images</code></strong></dt>
<dd>limit the number of image in the attr_dataset per epoch (before filtering)</dd>
<dt><strong><code>balance</code></strong></dt>
<dd>EnumBalance indicating the class used to balance images</dd>
<dt><strong><code>augmentations_img</code></strong></dt>
<dd>opt str, list of augmentations to apply separated by commas to apply to source image</dd>
<dt><strong><code>augmenter_img</code></strong></dt>
<dd>opt EnumAugmenter, name of the augmenter to use on source image</dd>
<dt><strong><code>augmentation_factor</code></strong></dt>
<dd>the number of replicas of the original attr_dataset to do</dd>
<dt><strong><code>label_modifier</code></strong></dt>
<dd>EnumLabelModifier</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClassificationPatch(BaseClass):
    &#34;&#34;&#34;Class that adapt the inputs from the hdf5 file (input image, label image), and manage other objects to create patches and
    filter them.

    Args:
        patch_creator: the object of PatchCreator0 class managing patches
        input_size: the size of the image provided as input to the attr_model ⚠️
        limit_num_images: limit the number of image in the attr_dataset per epoch (before filtering)
        balance: EnumBalance indicating the class used to balance images
        augmentations_img: opt str, list of augmentations to apply separated by commas to apply to source image
        augmenter_img: opt EnumAugmenter, name of the augmenter to use on source image
        augmentation_factor: the number of replicas of the original attr_dataset to do
        label_modifier: EnumLabelModifier
    &#34;&#34;&#34;
    def __init__(self, input_size: int = None,
                 limit_num_images: int = None, balance: EnumBalance = EnumBalance.NoBalance,
                 augmentations_img=&#34;none&#34;, augmenter_img: EnumAugmenter = EnumAugmenter.NoAugmenter,
                 augmentation_factor: int = 100, label_modifier: EnumLabelModifier = EnumLabelModifier.NoLabelModifier,
                 classes_to_use: Tuple[EnumClasses] = (EnumClasses.Seep, EnumClasses.Spill),
                 tr_percent=0.7, grid_size_px: int = 1000, threshold_margin:int = 1000):
        self.attr_name = self.__class__.__name__  # save the name of the class used for reproductibility purposes
        self.attr_global_name = &#34;attr_dataset&#34;
        self.attr_image_dataset, self.attr_label_dataset,self.dico_infos = FabricPreprocessedCache()()
        self.attr_grid_size_px = grid_size_px
        self.attr_limit_num_images = limit_num_images
        self.attr_check_margin_reject = MarginCheck(threshold=threshold_margin)
        self.attr_augmentation_factor = augmentation_factor
        with self.attr_image_dataset as images:
            self.datasets = {
                &#34;tr&#34;:list(images.keys())[:int(len(images) * tr_percent)],
                &#34;valid&#34;:list(images.keys())[int(len(images) * tr_percent):]
            }
        self.attr_global_name = &#34;attr_dataset&#34;
        if label_modifier == EnumLabelModifier.NoLabelModifier:
            self.attr_label_modifier = LabelModifier0(class_mapping=ClassificationPatch.attr_original_class_mapping)
        elif label_modifier == EnumLabelModifier.LabelModifier1:
            self.attr_label_modifier = LabelModifier1(classes_to_use=classes_to_use,
                                                      original_class_mapping=ClassificationPatch.attr_original_class_mapping)
        elif label_modifier == EnumLabelModifier.LabelModifier2:
            self.attr_label_modifier = LabelModifier2(classes_to_use=classes_to_use,
                                                      original_class_mapping=ClassificationPatch.attr_original_class_mapping)
        else:
            raise NotImplementedError(f&#34;{label_modifier} is not implemented&#34;)
        if balance == EnumBalance.NoBalance:
            self.attr_balance = NoBalance()
        elif balance == EnumBalance.BalanceClasses1:
            # see class DataSentinel1Segmentation for documentation on attr_class_mapping storage and access to values
            self.attr_balance = BalanceClasses1(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        elif balance == EnumBalance.BalanceClasses2:
            self.attr_balance = BalanceClasses2(other_index=self.attr_original_class_mapping[&#34;other&#34;])
        else:
            raise NotImplementedError
        if augmentations_img != &#34;none&#34;:
            if augmenter_img == EnumAugmenter.Augmenter1:
                self.attr_augmenter = Augmenter1(allowed_transformations=augmentations_img,
                                                 patch_size_before_final_resize=
                                                     self.attr_grid_size_px,
                                                 patch_size_final_resize=input_size
                                                 )

            else:
                self.attr_augmenter = NoAugmenter(allowed_transformations=augmentations_img,
                                                  patch_size_before_final_resize=
                                                     self.attr_grid_size_px,
                                                  patch_size_final_resize=input_size
                                                  )
        else:
            raise NotImplementedError(f&#34;{augmenter_img} is not implemented&#34;)
        # Cache to store between epochs rejected images if we have no image augmenter
        self.cache_img_id_rejected = []
    def set_datasets(self,image_dataset: ImageDataset, label_dataset: AbstractDataset, dico_infos: Dict):
        &#34;&#34;&#34;Change the origin of the patches

        Args:
            image_dataset: ImageDataset
            label_dataset: AbstractDataset Points or Images
            dico_infos: Dict, containing for each id of image the source image (under key source_img) and the transformation matrix (under key transformation_matrix) applied to get the patch
        Returns:

        &#34;&#34;&#34;
        self.attr_image_dataset = image_dataset
        self.attr_label_dataset = label_dataset
        self.dico_infos = dico_infos
    def __iter__(self, dataset: Union[EnumDataset,List[str]] = EnumDataset.Train):
        if isinstance(dataset,list):
            keys = dataset
        else:
            keys = self.datasets[dataset]
        return iter(self.generator(keys))

    def generator(self, images_available):
        &#34;&#34;&#34;

        Args:

        Returns:
            generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
            tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
                   classif: np.ndarray (shape (num_classes,), classification patch ;
                   transformation_matrix:  the transformation matrix to transform the source image
                   item: str name of the source image
        &#34;&#34;&#34;
        with self.attr_image_dataset as images_dataset:
            with self.attr_label_dataset as labels_dataset:
                for num_dataset in range(self.attr_augmentation_factor):
                    random.shuffle(images_available)
                    for item in images_available:
                        image = images_dataset.get(item)
                        image = np.array(image, dtype=np.float32)
                        polygons = labels_dataset.get(item)
                        partial_transformation_matrix = self.attr_augmenter.choose_new_augmentations(image)
                        for patch_upper_left_corner_coords in np.random.permutation(self.attr_augmenter.get_grid(image.shape, partial_transformation_matrix)):
                            annotations_patch, transformation_matrix = self.attr_augmenter.transform_label(
                                polygons=polygons,
                                partial_transformation_matrix=partial_transformation_matrix,
                                patch_upper_left_corner_coords=patch_upper_left_corner_coords
                            )
                            # Create the classification label with the proper technic
                            classification = self.attr_label_modifier.make_classification_label(annotations_patch)
                            balance_reject = self.attr_balance.filter(self.attr_label_modifier.get_initial_label())
                            if balance_reject is True:
                                continue
                            image_patch, transformation_matrix = self.attr_augmenter.transform_image(
                                image=image,
                                partial_transformation_matrix=partial_transformation_matrix,
                                patch_upper_left_corner_coords=patch_upper_left_corner_coords
                            )
                            reject = self.attr_check_margin_reject.check_reject(image_patch)
                            if reject is True:
                                continue
                            # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                            image_patch = np.stack((image_patch,)*3, axis=0)
                            yield image_patch, classification, transformation_matrix, item

    def get_patch(self,image: np.ndarray,annotation: np.ndarray, patch_upper_left_corner_coords: Tuple[int,int],
                  standardizer: AbstractStandardizer,label_encoding: TwoWayDict, transformation_matrix: Optional[np.ndarray] = None
                  ) -&gt; Tuple[np.ndarray,np.ndarray,np.ndarray]:
        &#34;&#34;&#34;Generate image patch and corresponding annotation for the given parameters

        Args:
            image: np.ndarray, image on which to apply the transformations
            annotation: np.ndarray, annotation on which to apply the transformations
            patch_upper_left_corner_coords: coordinates of the upper left corner to get
            standardizer: object giving allowing to standardize the patch
            label_encoding: TwoWayDict mapping between labels names and encoding as uint8
            transformation_matrix: Optional[np.ndarray] (3,3) transformation matrix to apply to the image and annotation ⚠️⚠️ no rotation allowed as we have an array annotation

        Returns:
            Tuple[np.ndarray,np.ndarray,np.ndarray]
            - image_patch: patch extracted from the image
            - classification: vector containing true probabilities of presence of annotation
            - transformation_matrix: 3,3 transformation matrix applied
        &#34;&#34;&#34;
        if transformation_matrix is None:
            transformation_matrix = np.identity(3)
        image_patch, transformation_matrix = self.attr_augmenter.transform_image(
            image=image,
            partial_transformation_matrix=transformation_matrix,
            patch_upper_left_corner_coords=patch_upper_left_corner_coords
        )
        annotation_patch, transformation_matrix = self.attr_augmenter.transform_image(
            image=annotation,
            partial_transformation_matrix=transformation_matrix,
            patch_upper_left_corner_coords=patch_upper_left_corner_coords
        )
        classification = self.attr_label_modifier.make_classification_label(annotation_patch)
        image_patch = np.stack((image_patch,)*3, axis=0)
        image_patch = standardizer.standardize(image_patch)
        return image_patch,classification,transformation_matrix
    def __len__(self):
        return None
    def set_standardizer(self, standardizer: AbstractStandardizer):
        self.attr_standardizer = standardizer
    def set_annotator(self,annotations):
        self.annotations_labels = annotations

    def len(self, dataset: str) -&gt; Optional[int]:
        return None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="main.src.param_savers.BaseClass.BaseClass" href="../../param_savers/BaseClass.html#main.src.param_savers.BaseClass.BaseClass">BaseClass</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.generator"><code class="name flex">
<span>def <span class="ident">generator</span></span>(<span>self, images_available)</span>
</code></dt>
<dd>
<div class="desc"><p>Args:</p>
<h2 id="returns">Returns</h2>
<dl>
<dt>generator of the attr_dataset (object that support <strong>iter</strong> and <strong>next</strong> magic methods)</dt>
<dt><code>tuple</code></dt>
<dd>input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
classif: np.ndarray (shape (num_classes,), classification patch ;
transformation_matrix:
the transformation matrix to transform the source image
item: str name of the source image</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generator(self, images_available):
    &#34;&#34;&#34;

    Args:

    Returns:
        generator of the attr_dataset (object that support __iter__ and __next__ magic methods)
        tuple: input: np.ndarray (shape (grid_size,grid_size,3)), input image for the attr_model ;
               classif: np.ndarray (shape (num_classes,), classification patch ;
               transformation_matrix:  the transformation matrix to transform the source image
               item: str name of the source image
    &#34;&#34;&#34;
    with self.attr_image_dataset as images_dataset:
        with self.attr_label_dataset as labels_dataset:
            for num_dataset in range(self.attr_augmentation_factor):
                random.shuffle(images_available)
                for item in images_available:
                    image = images_dataset.get(item)
                    image = np.array(image, dtype=np.float32)
                    polygons = labels_dataset.get(item)
                    partial_transformation_matrix = self.attr_augmenter.choose_new_augmentations(image)
                    for patch_upper_left_corner_coords in np.random.permutation(self.attr_augmenter.get_grid(image.shape, partial_transformation_matrix)):
                        annotations_patch, transformation_matrix = self.attr_augmenter.transform_label(
                            polygons=polygons,
                            partial_transformation_matrix=partial_transformation_matrix,
                            patch_upper_left_corner_coords=patch_upper_left_corner_coords
                        )
                        # Create the classification label with the proper technic
                        classification = self.attr_label_modifier.make_classification_label(annotations_patch)
                        balance_reject = self.attr_balance.filter(self.attr_label_modifier.get_initial_label())
                        if balance_reject is True:
                            continue
                        image_patch, transformation_matrix = self.attr_augmenter.transform_image(
                            image=image,
                            partial_transformation_matrix=partial_transformation_matrix,
                            patch_upper_left_corner_coords=patch_upper_left_corner_coords
                        )
                        reject = self.attr_check_margin_reject.check_reject(image_patch)
                        if reject is True:
                            continue
                        # convert the image to rgb (as required by pytorch): not ncessary the best transformation as we multiply by 3 the amount of data
                        image_patch = np.stack((image_patch,)*3, axis=0)
                        yield image_patch, classification, transformation_matrix, item</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.get_patch"><code class="name flex">
<span>def <span class="ident">get_patch</span></span>(<span>self, image: numpy.ndarray, annotation: numpy.ndarray, patch_upper_left_corner_coords: Tuple[int, int], standardizer: <a title="main.src.data.Standardizer.AbstractStandardizer.AbstractStandardizer" href="../Standardizer/AbstractStandardizer.html#main.src.data.Standardizer.AbstractStandardizer.AbstractStandardizer">AbstractStandardizer</a>, label_encoding: <a title="main.src.data.TwoWayDict.TwoWayDict" href="../TwoWayDict.html#main.src.data.TwoWayDict.TwoWayDict">TwoWayDict</a>, transformation_matrix: Union[numpy.ndarray, NoneType] = None) ‑> Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Generate image patch and corresponding annotation for the given parameters</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong></dt>
<dd>np.ndarray, image on which to apply the transformations</dd>
<dt><strong><code>annotation</code></strong></dt>
<dd>np.ndarray, annotation on which to apply the transformations</dd>
<dt><strong><code>patch_upper_left_corner_coords</code></strong></dt>
<dd>coordinates of the upper left corner to get</dd>
<dt><strong><code>standardizer</code></strong></dt>
<dd>object giving allowing to standardize the patch</dd>
<dt><strong><code>label_encoding</code></strong></dt>
<dd>TwoWayDict mapping between labels names and encoding as uint8</dd>
<dt><strong><code>transformation_matrix</code></strong></dt>
<dd>Optional[np.ndarray] (3,3) transformation matrix to apply to the image and annotation ⚠️⚠️ no rotation allowed as we have an array annotation</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple[np.ndarray,np.ndarray,np.ndarray]
- image_patch: patch extracted from the image
- classification: vector containing true probabilities of presence of annotation
- transformation_matrix: 3,3 transformation matrix applied</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_patch(self,image: np.ndarray,annotation: np.ndarray, patch_upper_left_corner_coords: Tuple[int,int],
              standardizer: AbstractStandardizer,label_encoding: TwoWayDict, transformation_matrix: Optional[np.ndarray] = None
              ) -&gt; Tuple[np.ndarray,np.ndarray,np.ndarray]:
    &#34;&#34;&#34;Generate image patch and corresponding annotation for the given parameters

    Args:
        image: np.ndarray, image on which to apply the transformations
        annotation: np.ndarray, annotation on which to apply the transformations
        patch_upper_left_corner_coords: coordinates of the upper left corner to get
        standardizer: object giving allowing to standardize the patch
        label_encoding: TwoWayDict mapping between labels names and encoding as uint8
        transformation_matrix: Optional[np.ndarray] (3,3) transformation matrix to apply to the image and annotation ⚠️⚠️ no rotation allowed as we have an array annotation

    Returns:
        Tuple[np.ndarray,np.ndarray,np.ndarray]
        - image_patch: patch extracted from the image
        - classification: vector containing true probabilities of presence of annotation
        - transformation_matrix: 3,3 transformation matrix applied
    &#34;&#34;&#34;
    if transformation_matrix is None:
        transformation_matrix = np.identity(3)
    image_patch, transformation_matrix = self.attr_augmenter.transform_image(
        image=image,
        partial_transformation_matrix=transformation_matrix,
        patch_upper_left_corner_coords=patch_upper_left_corner_coords
    )
    annotation_patch, transformation_matrix = self.attr_augmenter.transform_image(
        image=annotation,
        partial_transformation_matrix=transformation_matrix,
        patch_upper_left_corner_coords=patch_upper_left_corner_coords
    )
    classification = self.attr_label_modifier.make_classification_label(annotation_patch)
    image_patch = np.stack((image_patch,)*3, axis=0)
    image_patch = standardizer.standardize(image_patch)
    return image_patch,classification,transformation_matrix</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.len"><code class="name flex">
<span>def <span class="ident">len</span></span>(<span>self, dataset: str) ‑> Union[int, NoneType]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def len(self, dataset: str) -&gt; Optional[int]:
    return None</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.set_annotator"><code class="name flex">
<span>def <span class="ident">set_annotator</span></span>(<span>self, annotations)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_annotator(self,annotations):
    self.annotations_labels = annotations</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.set_datasets"><code class="name flex">
<span>def <span class="ident">set_datasets</span></span>(<span>self, image_dataset: <a title="main.src.data.Datasets.ImageDataset.ImageDataset" href="../Datasets/ImageDataset.html#main.src.data.Datasets.ImageDataset.ImageDataset">ImageDataset</a>, label_dataset: <a title="main.src.data.Datasets.AbstractDataset.AbstractDataset" href="../Datasets/AbstractDataset.html#main.src.data.Datasets.AbstractDataset.AbstractDataset">AbstractDataset</a>, dico_infos: Dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Change the origin of the patches</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image_dataset</code></strong></dt>
<dd>ImageDataset</dd>
<dt><strong><code>label_dataset</code></strong></dt>
<dd>AbstractDataset Points or Images</dd>
<dt><strong><code>dico_infos</code></strong></dt>
<dd>Dict, containing for each id of image the source image (under key source_img) and the transformation matrix (under key transformation_matrix) applied to get the patch</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_datasets(self,image_dataset: ImageDataset, label_dataset: AbstractDataset, dico_infos: Dict):
    &#34;&#34;&#34;Change the origin of the patches

    Args:
        image_dataset: ImageDataset
        label_dataset: AbstractDataset Points or Images
        dico_infos: Dict, containing for each id of image the source image (under key source_img) and the transformation matrix (under key transformation_matrix) applied to get the patch
    Returns:

    &#34;&#34;&#34;
    self.attr_image_dataset = image_dataset
    self.attr_label_dataset = label_dataset
    self.dico_infos = dico_infos</code></pre>
</details>
</dd>
<dt id="main.src.data.classification.ClassificationPatch.ClassificationPatch.set_standardizer"><code class="name flex">
<span>def <span class="ident">set_standardizer</span></span>(<span>self, standardizer: <a title="main.src.data.Standardizer.AbstractStandardizer.AbstractStandardizer" href="../Standardizer/AbstractStandardizer.html#main.src.data.Standardizer.AbstractStandardizer.AbstractStandardizer">AbstractStandardizer</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_standardizer(self, standardizer: AbstractStandardizer):
    self.attr_standardizer = standardizer</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="main.src.data.classification" href="index.html">main.src.data.classification</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch">ClassificationPatch</a></code></h4>
<ul class="two-column">
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.generator" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.generator">generator</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.get_patch" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.get_patch">get_patch</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.len" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.len">len</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.set_annotator" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.set_annotator">set_annotator</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.set_datasets" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.set_datasets">set_datasets</a></code></li>
<li><code><a title="main.src.data.classification.ClassificationPatch.ClassificationPatch.set_standardizer" href="#main.src.data.classification.ClassificationPatch.ClassificationPatch.set_standardizer">set_standardizer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>