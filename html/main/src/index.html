<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>main.src API documentation</title>
<meta name="description" content="⚠⚠⚠ With the automatic tool used to build this html documentation magic methods (`__init__`, `__call__`, ...) are not shown directly as other methods …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main.src</code></h1>
</header>
<section id="section-intro">
<p>⚠⚠⚠ With the automatic tool used to build this html documentation magic methods (<code>__init__</code>, <code>__call__</code>, &hellip;) are not shown directly as other methods. You need to expand the code to see them for the moment ⚠⚠⚠</p>
<h1 id="quickstart">Quickstart</h1>
<h2 id="automatic-configuration">Automatic configuration</h2>
<p>In this project the user can directly use the main_script file to train a desired model on a dataset.
To choose all of the options the user can provide parameters in the console or can create a custom Parser
with default value according to
his/her choice. An premade parser is provided in
the main/src/parsers module.</p>
<p>We can also want to modify some options for the system. That is why we can have the following questions:</p>
<p><strong>I.</strong>
How to supply my own dataset ?</p>
<p><strong>II.</strong>
How create patches outside of the system ?</p>
<p><strong>III.</strong>
How to create a custom training process ?</p>
<p><strong>IV.</strong>
How to predict on new patches ?</p>
<h2 id="i-how-to-supply-my-own-dataset">I. How to supply my own dataset</h2>
<p>The system requires 3 subdatasets to build a dataset: the image subdataset, the corresponding annotations (2d annotations)
and an information file.</p>
<h3 id="building-the-files">Building the files</h3>
<h3 id="images">Images</h3>
<p>They must be provided as np.ndarrays in an hdf5 file following the same method as for annotations</p>
<h4 id="annotations">Annotations</h4>
<p>Annotations can be provided as np.ndarrays or polygons respectively to store in an hdf5 or pickle file</p>
<h5 id="hdf5">HDF5</h5>
<p>The user has to create one hdf5 dataset (with the create_dataset function, cf hdf5 package doc) per image.
The name of the dataset must be an uniq id</p>
<h5 id="polygons">Polygons</h5>
<p>Stored under a dict form:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; {
...     &quot;id_image_1&quot;: [
...         polygon1,
...         polygon2,
...         ...
...     ]
... }
</code></pre>
<p>with polygon a dict with:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; polygon1 = {
...     &quot;label&quot;:&quot;...&quot;, #(ex: seep, spill...),
...     &quot;points&quot;:[(...,...),(...,...),...]
... }
</code></pre>
<h4 id="information-file-structure">Information file structure</h4>
<p>The information file is a json file containing at least:
- for a dataset to build patch (as for images_informations_preprocessed.json):
- under the key "transformation" the transformation matrix from geographic coordinates to pixel coordinates as proposed by rasterio</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; {
...     &quot;image_id1&quot;:{
...         &quot;transformation&quot;:...,
...         ...
...     },
...     ...
... }
</code></pre>
<ul>
<li>for a dataset of premade patches (as for filtered_img_infos.json):</li>
<li>under the key "transformation_matrix" the transformation matrix from the source image to the patch (with augmentations)</li>
<li>under the key "source_img": the id in the origin dataset from which the patch has been built</li>
</ul>
<pre><code class="language-python-repl">&gt;&gt;&gt; {
...     &quot;image_id1&quot;:{
...         &quot;transformation_matrix&quot;:...,
...         &quot;source_img&quot;:&quot;...&quot;
...     },
...     ...
... }
</code></pre>
<h3 id="managing-the-files-in-the-system">Managing the files in the system</h3>
<h4 id="1-build-subdatasets">1. Build subdatasets</h4>
<p>It is mandatory to create/use a class to manage the image and annotation subdataset.</p>
<p>The user can use premade classes
of the <a href="./data/Datasets/index.html">main/src/data/Datasets module</a> to manage hdf5 files or polygon annotations</p>
<p>The user can also build a custom dataset y following the structure of the <a href="./data/Datasets/AbstractDataset.html">AbstractDataset class</a></p>
<p>⚠⚠ go to the original python code to build your own as this documentation does not allow to show python magic functions. Some of them are required to implement for a custom dataset</p>
<h4 id="2-build-a-fabric">2. Build a fabric</h4>
<p>To associate the subdatasets together and open the json information file the user has to create a Fabric class creating all of them.</p>
<p>The expected structure is following the <a href="./data/Datasets/Fabrics/AbstractFabricDatasets.html">AbstractFabricDatasets class</a></p>
<h3 id="providing-it-to-the-system">Providing it to the system</h3>
<p>The user has first to provide the relevant arguments to the <a href="./data/DatasetFactory.html">DatasetFactory constructor</a> for the <a href="./data/classification/ClassificationGeneratorCache.html">ClassificationGeneratorCache</a>
or <a href="|./data/classification/ClassificationGeneratorPatch.html">ClassificationGeneratorPatch</a> class</p>
<p>The user can specify in this same constructor the algorithm to use with the <code>choose_dataset</code> argument and pass it the result
of the custom made Factory call function (the user can use arguments unpacking to use less lines of code)</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dataset_factory.set_datasets(*CustomDatasetFactory()())
</code></pre>
<h2 id="ii-how-to-create-patches-outside-of-the-system">II. How to create patches outside of the system</h2>
<p>The user can create patches on preopened image by using the <a href="./data/Augmentation/Augmenters/NoAugmenter.py">NoAugmenter</a> class</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; from main.src.data.Augmentation.Augmenters.NoAugmenter import NoAugmenter
&gt;&gt;&gt; augmenter = NoAugmenter(patch_size_before_final_resize=1000,patch_size_final_resize=256)
</code></pre>
<p>We can then get the partial transformation matrix for the size reduction:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; partial_transformation_matrix = augmenter.choose_new_augmentations(custom_image)
</code></pre>
<p>Then we have to choose the coordinates of the patch to extract.
For that we can get a grid (list of upper left corners) of patch by calling</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; grid = augmenter.get_grid(custom_image.shape,partial_transformation_matrix)
</code></pre>
<p>Then we can use the augmenter to apply the transformation to get the patch desired in the upper left corner of the view.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; augmenter.transform_image(custom_image,partial_transformation_matrix,patch_upper_left_corner_coords)[:256,:256]
</code></pre>
<h2 id="iii-how-to-create-a-custom-training-process">III. How to create a custom training process</h2>
<p>The user has a premade training object <a href="./training/Trainers/Trainer0.html">TrainerGenerateCache</a>.
We can provide it the DatasetFactory with a custom dataset, a custom model, a custom optimizer ....
more details in <a href="./training/Trainers/Trainer0.html">TrainerGenerateCache</a> constructor</p>
<p>The user can also build a custom trainer and using the object relevant to the goal pursued.</p>
<p>No specific structure is required. The user can iterator with a for loop over the dataset factory object (see TrainerGenerateCache example)
and has to keep in mind that the values produced by the Datasetfactory for loop correspond to individual samples, not batches.</p>
<p>It is the responsability of the Trainer object to build batches for the moment. You can use the TrainerGenerateCache structure as an example</p>
<h2 id="iv-how-to-build-and-predict-on-new-patches">IV. How to build and predict on new patches ?</h2>
<p>The user can build new patches by using the datasetfacotry and calling</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; dataset_factory.attr_dataset.get_patch(....)
patch_img,patch_annotation,transformation_matrix
</code></pre>
<p>The arguments then differs for the type of dataset create (ClassificationGeneratorCache or ClassificationGeneratorPatch)</p>
<p>Refers to their documentation to get specific arguments required</p>
<p>Then one can get the model of the ModelFactory by using its model property</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; my_pytorch_model = model_factory.model
</code></pre>
<p>and can then predict the result as usual in pytorch</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import torch
&gt;&gt;&gt; with torch.no_grad():
...     my_pytorch_model.eval()
...     input_gpu = torch.Tensor(patch_img).to(torch.device(&quot;cuda&quot;))
...     prediction = my_pytorch_model(input_gpu).cpu().detach().numpy()
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
⚠⚠⚠ With the automatic tool used to build this html documentation magic methods (`__init__`, `__call__`, ...) are not shown directly as other methods. You need to expand the code to see them for the moment ⚠⚠⚠
# Quickstart

## Automatic configuration

In this project the user can directly use the main_script file to train a desired model on a dataset.
To choose all of the options the user can provide parameters in the console or can create a custom Parser
with default value according to  his/her choice. An premade parser is provided in  the main/src/parsers module.

We can also want to modify some options for the system. That is why we can have the following questions:

**I.**      How to supply my own dataset ?

**II.**     How create patches outside of the system ?

**III.**    How to create a custom training process ?

**IV.**     How to predict on new patches ?

## I. How to supply my own dataset

The system requires 3 subdatasets to build a dataset: the image subdataset, the corresponding annotations (2d annotations)
and an information file.


### Building the files

### Images

They must be provided as np.ndarrays in an hdf5 file following the same method as for annotations


#### Annotations

Annotations can be provided as np.ndarrays or polygons respectively to store in an hdf5 or pickle file

##### HDF5

The user has to create one hdf5 dataset (with the create_dataset function, cf hdf5 package doc) per image.
The name of the dataset must be an uniq id

##### Polygons

Stored under a dict form:

&gt;&gt;&gt; {
...     &#34;id_image_1&#34;: [
...         polygon1,
...         polygon2,
...         ...
...     ]
... }

with polygon a dict with:

&gt;&gt;&gt; polygon1 = {
...     &#34;label&#34;:&#34;...&#34;, #(ex: seep, spill...),
...     &#34;points&#34;:[(...,...),(...,...),...]
... }

#### Information file structure

The information file is a json file containing at least:
- for a dataset to build patch (as for images_informations_preprocessed.json):
  - under the key &#34;transformation&#34; the transformation matrix from geographic coordinates to pixel coordinates as proposed by rasterio

&gt;&gt;&gt; {
...     &#34;image_id1&#34;:{
...         &#34;transformation&#34;:...,
...         ...
...     },
...     ...
... }

- for a dataset of premade patches (as for filtered_img_infos.json):
  - under the key &#34;transformation_matrix&#34; the transformation matrix from the source image to the patch (with augmentations)
  - under the key &#34;source_img&#34;: the id in the origin dataset from which the patch has been built

&gt;&gt;&gt; {
...     &#34;image_id1&#34;:{
...         &#34;transformation_matrix&#34;:...,
...         &#34;source_img&#34;:&#34;...&#34;
...     },
...     ...
... }

### Managing the files in the system

#### 1. Build subdatasets

It is mandatory to create/use a class to manage the image and annotation subdataset.

The user can use premade classes  of the [main/src/data/Datasets module](./data/Datasets/index.html) to manage hdf5 files or polygon annotations

The user can also build a custom dataset y following the structure of the [AbstractDataset class](./data/Datasets/AbstractDataset.html)

⚠⚠ go to the original python code to build your own as this documentation does not allow to show python magic functions. Some of them are required to implement for a custom dataset


#### 2. Build a fabric

To associate the subdatasets together and open the json information file the user has to create a Fabric class creating all of them.

The expected structure is following the [AbstractFabricDatasets class](./data/Datasets/Fabrics/AbstractFabricDatasets.html)

### Providing it to the system


The user has first to provide the relevant arguments to the [DatasetFactory constructor](./data/DatasetFactory.html) for the [ClassificationGeneratorCache](./data/classification/ClassificationGeneratorCache.html)
or [ClassificationGeneratorPatch](|./data/classification/ClassificationGeneratorPatch.html) class

The user can specify in this same constructor the algorithm to use with the `choose_dataset` argument and pass it the result
of the custom made Factory call function (the user can use arguments unpacking to use less lines of code)

&gt;&gt;&gt; dataset_factory.set_datasets(*CustomDatasetFactory()())


## II. How to create patches outside of the system

The user can create patches on preopened image by using the [NoAugmenter](./data/Augmentation/Augmenters/NoAugmenter.py) class

&gt;&gt;&gt; from main.src.data.Augmentation.Augmenters.NoAugmenter import NoAugmenter
&gt;&gt;&gt; augmenter = NoAugmenter(patch_size_before_final_resize=1000,patch_size_final_resize=256)

We can then get the partial transformation matrix for the size reduction:

&gt;&gt;&gt; partial_transformation_matrix = augmenter.choose_new_augmentations(custom_image)


Then we have to choose the coordinates of the patch to extract.
For that we can get a grid (list of upper left corners) of patch by calling

&gt;&gt;&gt; grid = augmenter.get_grid(custom_image.shape,partial_transformation_matrix)

Then we can use the augmenter to apply the transformation to get the patch desired in the upper left corner of the view.

&gt;&gt;&gt; augmenter.transform_image(custom_image,partial_transformation_matrix,patch_upper_left_corner_coords)[:256,:256]

## III. How to create a custom training process

The user has a premade training object [TrainerGenerateCache](./training/Trainers/TrainerGenerateCache.html).
We can provide it the DatasetFactory with a custom dataset, a custom model, a custom optimizer ....
more details in [TrainerGenerateCache](./training/Trainers/TrainerGenerateCache.html) constructor

The user can also build a custom trainer and using the object relevant to the goal pursued.

No specific structure is required. The user can iterator with a for loop over the dataset factory object (see TrainerGenerateCache example)
and has to keep in mind that the values produced by the Datasetfactory for loop correspond to individual samples, not batches.

It is the responsability of the Trainer object to build batches for the moment. You can use the TrainerGenerateCache structure as an example

## IV. How to build and predict on new patches ?

The user can build new patches by using the datasetfacotry and calling

&gt;&gt;&gt; dataset_factory.attr_dataset.get_patch(....)
patch_img,patch_annotation,transformation_matrix

The arguments then differs for the type of dataset create (ClassificationGeneratorCache or ClassificationGeneratorPatch)

Refers to their documentation to get specific arguments required

Then one can get the model of the ModelFactory by using its model property

&gt;&gt;&gt; my_pytorch_model = model_factory.model

and can then predict the result as usual in pytorch
&gt;&gt;&gt; import torch
&gt;&gt;&gt; with torch.no_grad():
...     my_pytorch_model.eval()
...     input_gpu = torch.Tensor(patch_img).to(torch.device(&#34;cuda&#34;))
...     prediction = my_pytorch_model(input_gpu).cpu().detach().numpy()
&#34;&#34;&#34;</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="main.src.analysis" href="analysis/index.html">main.src.analysis</a></code></dt>
<dd>
<div class="desc"><p>Module that gathers scripts used to analyse data and analyse results …</p></div>
</dd>
<dt><code class="name"><a title="main.src.data" href="data/index.html">main.src.data</a></code></dt>
<dd>
<div class="desc"><p>Module that gathers classes to manage data</p></div>
</dd>
<dt><code class="name"><a title="main.src.enums" href="enums.html">main.src.enums</a></code></dt>
<dd>
<div class="desc"><p>Contains enumerations EnumGitCheck</p></div>
</dd>
<dt><code class="name"><a title="main.src.main_script" href="main_script.html">main.src.main_script</a></code></dt>
<dd>
<div class="desc"><p>Script to launch with(out) arguments to train the attr_model and save the results</p></div>
</dd>
<dt><code class="name"><a title="main.src.models" href="models/index.html">main.src.models</a></code></dt>
<dd>
<div class="desc"><p>Module that gathers classes to create and manage models</p></div>
</dd>
<dt><code class="name"><a title="main.src.param_savers" href="param_savers/index.html">main.src.param_savers</a></code></dt>
<dd>
<div class="desc"><p>Module containing classes that allow to sae parameters</p></div>
</dd>
<dt><code class="name"><a title="main.src.parsers" href="parsers/index.html">main.src.parsers</a></code></dt>
<dd>
<div class="desc"><p>Module that gathers all parsers available depending of the usecase</p></div>
</dd>
<dt><code class="name"><a title="main.src.training" href="training/index.html">main.src.training</a></code></dt>
<dd>
<div class="desc"><p>Classes used create the training loops and data transfer but also the computation of the loss and metrics thanks to the optimizer</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#quickstart">Quickstart</a><ul>
<li><a href="#automatic-configuration">Automatic configuration</a></li>
<li><a href="#i-how-to-supply-my-own-dataset">I. How to supply my own dataset</a><ul>
<li><a href="#building-the-files">Building the files</a></li>
<li><a href="#images">Images</a><ul>
<li><a href="#annotations">Annotations</a><ul>
<li><a href="#hdf5">HDF5</a></li>
<li><a href="#polygons">Polygons</a></li>
</ul>
</li>
<li><a href="#information-file-structure">Information file structure</a></li>
</ul>
</li>
<li><a href="#managing-the-files-in-the-system">Managing the files in the system</a><ul>
<li><a href="#1-build-subdatasets">1. Build subdatasets</a></li>
<li><a href="#2-build-a-fabric">2. Build a fabric</a></li>
</ul>
</li>
<li><a href="#providing-it-to-the-system">Providing it to the system</a></li>
</ul>
</li>
<li><a href="#ii-how-to-create-patches-outside-of-the-system">II. How to create patches outside of the system</a></li>
<li><a href="#iii-how-to-create-a-custom-training-process">III. How to create a custom training process</a></li>
<li><a href="#iv-how-to-build-and-predict-on-new-patches">IV. How to build and predict on new patches ?</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="main" href="../index.html">main</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="main.src.analysis" href="analysis/index.html">main.src.analysis</a></code></li>
<li><code><a title="main.src.data" href="data/index.html">main.src.data</a></code></li>
<li><code><a title="main.src.enums" href="enums.html">main.src.enums</a></code></li>
<li><code><a title="main.src.main_script" href="main_script.html">main.src.main_script</a></code></li>
<li><code><a title="main.src.models" href="models/index.html">main.src.models</a></code></li>
<li><code><a title="main.src.param_savers" href="param_savers/index.html">main.src.param_savers</a></code></li>
<li><code><a title="main.src.parsers" href="parsers/index.html">main.src.parsers</a></code></li>
<li><code><a title="main.src.training" href="training/index.html">main.src.training</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>